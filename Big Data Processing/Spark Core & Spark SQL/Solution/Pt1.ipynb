{"cells":[{"cell_type":"code","source":["# ------------------------------------------\n# Big Data Processing\n# Assignment 1 - Spark Core & Spark SQL\n# Part 1 - Spark Core\n# Michael McAleer R00143621\n# ------------------------------------------\nimport pyspark\n\n\ndef process_line(line):\n    \"\"\"Process a line from the input data rdd, cleans and returns tuple of\n    values.\n\n    :param line: raw line data -- str\n    :return: line values -- tuple (str * len(params))\n    \"\"\"\n    # Remove new line character\n    line = line.replace('\\n', '')\n    # Split line on delimiter\n    params = line.split(';')\n    # If the param count is 7 return tuple of line values else empty tuple\n    return tuple(params) if len(params) == 7 else ()\n\n\n###############################################################################\n\ndef ex1(rdd):\n    \"\"\"Exercise 1: Total amount of entries in the dataset.\n\n    :param rdd: processed input rdd -- pyspark rdd\n    \"\"\"\n    # Count the total amount of entries in the rdd and print\n    print('- Total dataset entries: {c}'.format(c=rdd.count()))\n\n\n###############################################################################\n\ndef ex2(rdd):\n    \"\"\"Exercise 2: Number of Coca-cola bikes stations in Cork.\n\n    :param rdd: processed input rdd -- pyspark rdd\n    \"\"\"\n    # Extract the station names from entire rdd\n    extracted_rdd = rdd.map(lambda x: x[1])\n    # Get all the unique values from the rdd\n    distinct_rdd = extracted_rdd.distinct()\n    # Output the amount of unique values\n    print('- Total bike stations in Cork: {c}'.format(c=distinct_rdd.count()))\n\n\n###############################################################################\n\ndef ex3(rdd):\n    \"\"\"Exercise 3: List of Coca-Cola bike stations.\n\n    Note: It would be more efficient to put this in ex2() and collect from\n    there so the distinct value count does not need to be computed twice. In a\n    real world scenario this approach would be taken.\n\n    :param rdd: processed input rdd -- pyspark rdd\n    \"\"\"\n    # Extract the station names from entire rdd\n    extracted_rdd = rdd.map(lambda x: x[1])\n    # Get all the unique elements from the rdd\n    distinct_rdd = extracted_rdd.distinct()\n    # Collect all the unique elements into a list\n    result = distinct_rdd.collect()\n    # Print each of the elements individually\n    print('Cork Bike Station List:')\n    for n, val in enumerate(result):\n        print('- {i}: {station}'.format(i=n + 1, station=val))\n\n\n###############################################################################\n\ndef ex4(rdd):\n    \"\"\"Exercise 4: Sort the bike stations by their longitude (East to West).\n\n    :param rdd: processed input rdd -- pyspark rdd\n    \"\"\"\n    # Extract the station name and longitude from the input rdd\n    extracted_rdd = rdd.map(lambda x: tuple([x[1], float(x[2])]))\n    # Get the unique elements from the rdd\n    distinct_rdd = extracted_rdd.distinct()\n    # Sort the stations by longitude from East to West (descending)\n    result = distinct_rdd.sortBy(lambda x: x[1], ascending=False).collect()\n    # Print each of the elements\n    print('Cork Bike Stations (East -> West):')\n    for val in result:\n        print('- {station}: {pos}'.format(station=val[0], pos=val[1]))\n\n\n###############################################################################\n\ndef ex5(rdd):\n    \"\"\"Exercise 5: Average number of bikes available at Kent Station.\n\n    :param rdd: processed input rdd -- pyspark rdd\n    \"\"\"\n    # Filter the input rdd to extract only elements which match 'Kent Station'\n    filter_rdd = rdd.filter(\n        lambda x: True if x[1] == 'Kent Station' else False)\n    # Transform the rdd to float bike counts\n    bike_cnt_rdd = filter_rdd.map(lambda x: float(x[5]))\n    # Aggregate the bike counts and keep count of elements processed\n    result = bike_cnt_rdd.aggregate(\n        (0, 0),\n        lambda x, y: tuple([(x[0] + y), (x[1] + 1)]),\n        lambda x, y: tuple([(x[0] + y[0]), x[1] + y[1]]))\n    # Print the results\n    print('- Total sum of bikes: {s}'.format(s=int(result[0])))\n    print('- Total intervals: {i}'.format(i=result[1]))\n    print('- Average bike count: {a}'.format(a=result[0] / result[1]))\n\n\n###############################################################################\n\nif __name__ == '__main__':\n    # Set the location of the dataset directory\n    FILE_STORE = '/FileStore/tables/cork_bike_data'\n    # Configure the Spark Context\n    sc = pyspark.SparkContext.getOrCreate()\n    # Set log level\n    sc.setLogLevel('WARN')\n    # Load the dataset into a RDD\n    raw_rdd = sc.textFile('{data}/*.csv'.format(data=FILE_STORE))\n    # Process each line to get the relevant info as a tuple of values\n    input_rdd = raw_rdd.map(process_line)\n    # Persist the RDD to memory for re-use\n    input_rdd.cache()\n    # Call the functions\n    for i, ex in enumerate([ex1, ex2, ex3, ex4, ex5]):\n        print('\\n#-------------#\\n'\n              '| Exercise: {e} |\\n'\n              '#-------------#'.format(e=i + 1))\n        ex(input_rdd)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\n#-------------#\n Exercise: 1 |\n#-------------#\n- Total dataset entries: 1339200\n\n#-------------#\n Exercise: 2 |\n#-------------#\n- Total bike stations in Cork: 31\n\n#-------------#\n Exercise: 3 |\n#-------------#\nCork Bike Station List:\n- 1: North Main St.\n- 2: Coburg St.\n- 3: Fitzgerald&apos;s Park\n- 4: Camden Quay\n- 5: Father Mathew Statue\n- 6: College of Commerce\n- 7: South Mall\n- 8: Wandesford Quay\n- 9: South Gate Bridge\n- 10: St. Fin Barre&apos;s Bridge\n- 11: Emmet Place\n- 12: Brian Boru Bridge\n- 13: Lapp&apos;s Quay\n- 14: Dyke Parade\n- 15: South Main St.\n- 16: Bandfield\n- 17: St. Patricks St.\n- 18: Gaol Walk\n- 19: Corn Market St.\n- 20: Peace Park\n- 21: Clontarf Street\n- 22: Cork School of Music\n- 23: Mercy Hospital\n- 24: Bishop St.\n- 25: Pope&apos;s Quay\n- 26: Grattan St.\n- 27: Cork City Hall\n- 28: Kent Station\n- 29: Grand Parade\n- 30: Lower Glanmire Rd.\n- 31: Bus Station\n\n#-------------#\n Exercise: 4 |\n#-------------#\nCork Bike Stations (East -&gt; West):\n- Kent Station: -8.45821512\n- Lower Glanmire Rd.: -8.46411816\n- Brian Boru Bridge: -8.465153\n- Clontarf Street: -8.46562933177544\n- Lapp&apos;s Quay: -8.465735\n- Cork City Hall: -8.466\n- Bus Station: -8.46695074\n- Cork School of Music: -8.46809252166047\n- College of Commerce: -8.469797\n- South Mall: -8.46989982762231\n- Coburg St.: -8.47056736\n- Father Mathew Statue: -8.4706278\n- St. Patricks St.: -8.47261531\n- Emmet Place: -8.47270466388061\n- Camden Quay: -8.473342\n- Peace Park: -8.47347588279142\n- Grand Parade: -8.47536977381303\n- South Gate Bridge: -8.47586514429047\n- South Main St.: -8.47689553164735\n- Corn Market St.: -8.477\n- Pope&apos;s Quay: -8.477385\n- North Main St.: -8.47844005\n- Bishop St.: -8.4790268\n- Grattan St.: -8.47977966\n- Wandesford Quay: -8.48004\n- St. Fin Barre&apos;s Bridge: -8.48196155\n- Mercy Hospital: -8.48225676\n- Dyke Parade: -8.48458467\n- Bandfield: -8.4891363\n- Fitzgerald&apos;s Park: -8.49341266\n- Gaol Walk: -8.494174\n\n#-------------#\n Exercise: 5 |\n#-------------#\n- Total sum of bikes: 553407\n- Total intervals: 43200\n- Average bike count: 12.810347222222223\n</div>"]}}],"execution_count":1}],"metadata":{"name":"Pt1","notebookId":1226440966361976},"nbformat":4,"nbformat_minor":0}
