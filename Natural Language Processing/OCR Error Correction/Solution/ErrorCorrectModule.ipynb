{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ErrorCorrectModule.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"plOLk5ahG8LY","colab_type":"text"},"source":["# Notes for the user\n","*Creator: Michael McAleer*\n","*SN: R00143621*\n","\n","This notebook assumes that it is being run on Google CoLab. To run locally, change the path of the `root_dir` to your current working directory, the rest of the required paths are determined from there. You can find `root_dir` on line 6 of the section 'Data Path Configuration'.\n","\n","All required files are included with this submission, it should work as-is out of the box when the `root_dir` has been set to current working directory.\n","\n","The main corpus used in this notebook is `ukenglish.txt` which is a list of\n","approx. 80,000 unique english words, it can be found [here](http://www.gwicks.net/dictionaries.htm).\n","\n","Although the notebook uses a good corpus in terms of data quality, this notebook has the ability to read from a directory and ingest all text based documents within, combining them into a single corpus before processing and cleaning.\n","\n","An attempt was made to save the model mapping and associated weights but an issue was encounteres where a loaded model was peforming like it had never been trained. A related GitHub article can be found [here](https://github.com/keras-team/keras/issues/4875)."]},{"cell_type":"markdown","metadata":{"id":"VjH3Fdo9ROK5","colab_type":"text"},"source":["# Imports Libraries & Install Packages"]},{"cell_type":"code","metadata":{"id":"LqehXQomixwZ","colab_type":"code","outputId":"a614fec9-2d1e-4079-a062-f8ea0d70adfe","executionInfo":{"status":"ok","timestamp":1572192247861,"user_tz":0,"elapsed":29624,"user":{"displayName":"Michael McAleer","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBA484SnEl51AELTFepW1wp9_GgYnxoLyFNlRtVUg=s64","userId":"03089754270101753013"}},"colab":{"base_uri":"https://localhost:8080/","height":133}},"source":["from __future__ import print_function\n","\n","import os\n","import re\n","import string\n","import time\n","\n","import numpy as np\n","import tensorflow as tf\n","\n","from copy import deepcopy\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","from nltk.tokenize import wordpunct_tokenize\n","from numpy.random import choice\n","from numpy.random import rand\n","from numpy.random import randint\n","\n","# To run pycontractions in CoLab JDK needs to be downgraded to version 8\n","print('Downgrading open-jdk to version 8 for pycontractions install...')\n","!apt-get purge openjdk* -qq > /dev/null\n","!apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n","!pip install --quiet pycontractions\n","print('pycontractions install complete')\n","from pycontractions import Contractions\n","\n","# Ignore TensorFlow warnings\n","tf.logging.set_verbosity(tf.logging.ERROR)"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Downgrading open-jdk to version 8 for pycontractions install...\n","pycontractions install complete\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rB6swBYQRRx8","colab_type":"text"},"source":["# Data Path Configuration"]},{"cell_type":"code","metadata":{"id":"TL9aPJY9i466","colab_type":"code","outputId":"0714bcae-02a8-4220-a021-4a7b80483728","executionInfo":{"status":"ok","timestamp":1572192247862,"user_tz":0,"elapsed":29606,"user":{"displayName":"Michael McAleer","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBA484SnEl51AELTFepW1wp9_GgYnxoLyFNlRtVUg=s64","userId":"03089754270101753013"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# Mount gDrive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Path to working director - must be set\n","root_dir = '/content/drive/My Drive/Colab Notebooks/NLP/Assignment1'\n","\n","# Path to the data files\n","corpus_dir = '{root_dir}/corpora'.format(root_dir=root_dir)\n","training_dir = '{corpus_root}/training'.format(corpus_root=corpus_dir)\n","validation_dir = '{corpus_root}/validation'.format(corpus_root=corpus_dir)\n","model_dir = '{root_dir}/models'.format(root_dir=root_dir)\n","data_path = '{train}/ukenglish.txt'.format(train=training_dir)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HGlo9n1xiu-a","colab_type":"text"},"source":["# OCR Baseline System"]},{"cell_type":"code","metadata":{"id":"TUcBaxtaNvCA","colab_type":"code","colab":{}},"source":["# Provided OCR cannot read whole sentences, a lot of time was wasted tring to\n","# implement this but was ultimately left out as it is not a defined task in the\n","# assignment.  Feeding in a dummy sentence with errors and contractions. The OCR\n","# system has been included along with this assignment to show it is working for\n","# single word prediction and an attempt was made to get it working.\n","ocr_output_string = (\"We're goinng to the zoo and I don't thnk I'll be \"\n","                     \"home for dnner\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c01rZv-8mI-x","colab_type":"text"},"source":["# Task 1 - De-Contraction of String using PyContractions"]},{"cell_type":"code","metadata":{"id":"d5CG6PZJnC_Q","colab_type":"code","outputId":"f7332172-012e-4f4c-f1c5-75970c8a760d","executionInfo":{"status":"ok","timestamp":1572192275914,"user_tz":0,"elapsed":57634,"user":{"displayName":"Michael McAleer","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBA484SnEl51AELTFepW1wp9_GgYnxoLyFNlRtVUg=s64","userId":"03089754270101753013"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["# Load semantic vector model\n","py_cont = Contractions(api_key=\"glove-wiki-gigaword-50\")\n","# Prevent loading on first expand_texts call\n","py_cont.load_models()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"VTTQaY8InEZr","colab_type":"code","outputId":"775e2b66-a53d-4e17-9a41-b61790a3a4d7","executionInfo":{"status":"ok","timestamp":1572192278246,"user_tz":0,"elapsed":59953,"user":{"displayName":"Michael McAleer","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBA484SnEl51AELTFepW1wp9_GgYnxoLyFNlRtVUg=s64","userId":"03089754270101753013"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Expand contractions from out OCR output sentence\n","decont_ocr_output = list(py_cont.expand_texts([ocr_output_string]))\n","# We are only feeding in one sentence so need to extract string from list\n","decont_ocr_output = decont_ocr_output[0]\n","# Output contraction success to screen with comparison against original\n","print('OCR output sentence: {s}'.format(s=ocr_output_string))\n","print('Decontracted sentence: {s}'.format(s=decont_ocr_output))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["OCR output sentence: We're goinng to the zoo and I don't thnk I'll be home for dnner\n","Decontracted sentence: we are goinng to the zoo and I do not thnk I will be home for dnner\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AwlgIpUfnGgF","colab_type":"text"},"source":["# Task 2 - Tokenise De-Contracted String with NLTK"]},{"cell_type":"code","metadata":{"id":"_A5GYXVNnIB3","colab_type":"code","outputId":"48865807-d995-46b7-c47e-21dd99ec17a6","executionInfo":{"status":"ok","timestamp":1572192278247,"user_tz":0,"elapsed":59941,"user":{"displayName":"Michael McAleer","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBA484SnEl51AELTFepW1wp9_GgYnxoLyFNlRtVUg=s64","userId":"03089754270101753013"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# Using NLTK tokenise the de-contracted OCR output sentence, this is more\n","# efficient than just splitting on whitespace as it has awareness for\n","# punctuation and hyphenated words\n","ocr_tokens = wordpunct_tokenize(decont_ocr_output)\n","print('Tokenised de-contracted sentence: {s}'.format(s=ocr_tokens))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Tokenised de-contracted sentence: ['we', 'are', 'goinng', 'to', 'the', 'zoo', 'and', 'I', 'do', 'not', 'thnk', 'I', 'will', 'be', 'home', 'for', 'dnner']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W_rjLlOtnTcE","colab_type":"text"},"source":["# Task 3 - Detecting Errors"]},{"cell_type":"markdown","metadata":{"id":"3KdodgV6SyKN","colab_type":"text"},"source":["## Task 3.1 - Check if OCR output tokens are english words"]},{"cell_type":"code","metadata":{"id":"vwgqcrvFneh-","colab_type":"code","colab":{}},"source":["# Define the set of valid chars and invalid punctuation regex pattern \n","CHARS = list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n","REMOVE_CHARS = r'[#$%\"\\+@<=>!&,-.?:;()*\\[\\]^_`{|}~/\\d\\t\\n\\r\\x0b\\x0c“”]'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uKtmICYXniYm","colab_type":"code","colab":{}},"source":["def load_corpus_data(corpus_directory):\n","    \"\"\"Given a corpus directory, load all files within and return as one text.\n","\n","    :param corpus_directory: path to director -- str\n","    :return: concatenated corpora -- str\n","    \"\"\"\n","    corpus_files = os.listdir(corpus_directory)\n","    text = str()\n","    for book in corpus_files:\n","        book_path = os.path.join(corpus_directory, book)\n","        with open(book_path, 'r', encoding='utf8') as open_corpus:\n","            c_text = open_corpus.read()\n","            text += c_text\n","    return text\n","\n","\n","def tokenise_corpus(in_text):\n","    \"\"\"Convert a string into tokens splitting on white space and remove all\n","    punctuation listed in REMOVE_CHARS regex pattern.\n","\n","    :param in_text: input text -- str\n","    :return: tokens -- list\n","    \"\"\"\n","    return [re.sub(REMOVE_CHARS, '', token) for token in (\n","        re.split(\"[-\\n ]\", in_text))]\n","\n","\n","def clean_text(in_tokens):\n","    \"\"\"Clean a list of tokens generated from corpus.\n","\n","    :param in_tokens: input tokens -- list\n","    :return: cleaned tokens -- list\n","    \"\"\"\n","    # Remove non-english words\n","    english_words = [c for c in in_tokens if _is_english_chars(c)]\n","    # Remove any words with that have anything other than alpha chars\n","    alpha_words = [c for c in english_words if re.match(r'[a-zA-Z]', c)]\n","    # Normalise text by changing it all to lower case\n","    normal_text = [w.lower() for w in alpha_words]\n","    return list(filter(None, set(normal_text)))\n","\n","\n","def _is_english_chars(in_string):\n","    \"\"\"Check if an input string consists of non-english characters.\n","    \n","    :param in_string: input string -- str\n","    :return: bool\n","    \"\"\"\n","    try:\n","        in_string.encode(encoding='utf-8').decode('ascii')\n","    except UnicodeDecodeError:\n","        return False\n","    else:\n","        return True\n","\n","\n","def _is_english_word(in_word, corpus):\n","    \"\"\"Perform set lookup on corpus to check if word exists. Calculates in\n","    O(1) time.\n","    \n","    :param in_word: input word -- str\n","    :param corpus: english words corpus -- set\n","    :return: bool\n","    \"\"\"\n","    return in_word.lower() in corpus"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"12PjVfZWnksN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"outputId":"c849a37c-1933-40d5-961f-d53c0651ef2b","executionInfo":{"status":"ok","timestamp":1572192278849,"user_tz":0,"elapsed":60507,"user":{"displayName":"Michael McAleer","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBA484SnEl51AELTFepW1wp9_GgYnxoLyFNlRtVUg=s64","userId":"03089754270101753013"}}},"source":["# Start corpus processing timer\n","p_start = time.time()\n","# Load corpora\n","corpus_text = load_corpus_data(training_dir)\n","# Tokenise corpus\n","corpus_tokens = tokenise_corpus(corpus_text)\n","# Clean corpus tokens\n","corpus_tokens = clean_text(corpus_tokens)\n","# Calculate unique character count in entire corpus\n","corpus_unique_chars = sorted(set(' '.join(corpus_tokens)))\n","# Calculate the longest token in the corpus\n","corpus_max_len = max([len(token) for token in corpus_tokens]) + 2\n","# Create corpus set for hash lookup\n","corpus_lookup_set = set(corpus_tokens)\n","\n","print('Total corpus processing time: {t}'.format(t=time.time() - p_start))\n","print('Corpus size: {s}'.format(s=len(corpus_tokens)))\n","print('Corpus unique character count: {c}'.format(c=len(corpus_unique_chars)))\n","print('Corpus largest word: {size}'.format(size=corpus_max_len))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Total corpus processing time: 0.2998542785644531\n","Corpus size: 82036\n","Corpus unique character count: [' ', \"'\", 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n","Corpus largest word: 23\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sO9dAuM_nm11","colab_type":"code","outputId":"8963c477-7937-4dcb-9996-fad16f690536","executionInfo":{"status":"ok","timestamp":1572192278849,"user_tz":0,"elapsed":60494,"user":{"displayName":"Michael McAleer","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBA484SnEl51AELTFepW1wp9_GgYnxoLyFNlRtVUg=s64","userId":"03089754270101753013"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["incorrect_words = list()\n","# For each of the OCR tokens...\n","for word in ocr_tokens:\n","    # Check if the word exists in the corpus...\n","    if not _is_english_word(word, corpus_lookup_set):\n","        # If not add to incorrect word list\n","        incorrect_words.append(word)\n","\n","print('Incorrect words: {w}'.format(w=incorrect_words))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Incorrect words: ['goinng', 'thnk', 'dnner']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DO25giuytO4S","colab_type":"text"},"source":["# LSTM Char-to-Char Sequence Learner "]},{"cell_type":"code","metadata":{"id":"Q69we2I-6qzn","colab_type":"code","colab":{}},"source":["# Reset TensorFlow default graph\n","tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nT3H6aDCv9q_","colab_type":"code","colab":{}},"source":["# LSTM configuration options\n","# Start of sequence\n","SOS = '$'\n","# End of sequence  \n","EOS = '*'\n","BATCH_SIZE = 1024\n","N_EPOCHS = 70\n","HIDDEN_DIM = 256"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HsrKQ3g9tZ6n","colab_type":"text"},"source":["## Prepare the training data"]},{"cell_type":"code","metadata":{"id":"lpaM0mROtZJH","colab_type":"code","colab":{}},"source":["def _add_noise_to_token(tkn, noise_rate):\n","    \"\"\"Add noise to token to simulate spelling mistakes.\n","\n","    :param tkn: input token -- str\n","    :param noise_rate: noise rate -- float\n","    :return: noise sampled token -- str\n","    \"\"\"\n","    # Do not run transform unless token is 3 or more chars in length\n","    if len(tkn) < 3:\n","        return tkn\n","    # Use only lower-case ASCII characters for noise\n","    chars = list(string.ascii_lowercase)\n","    # There are four ways in which noise can be generated, divide error rate\n","    # to provide equal chance for each change possible\n","    ran = rand()\n","    prob = noise_rate / 4.0\n","    # Replace a character with a random character\n","    if ran < prob:\n","        rci = randint(len(tkn))\n","        token = tkn[:rci] + choice(CHARS) + tkn[rci + 1:]\n","    # Delete a character\n","    elif prob < ran < prob * 2:\n","        rci = randint(len(tkn))\n","        tkn = tkn[:rci] + tkn[rci + 1:]\n","    # Add a random character\n","    elif prob * 2 < ran < prob * 3:\n","        rci = randint(len(tkn))\n","        tkn = tkn[:rci] + np.random.choice(CHARS) + tkn[rci:]\n","    # Transpose 2 characters\n","    elif prob * 3 < ran < prob * 4:\n","        rci = randint(len(tkn) - 1)\n","        token = tkn[:rci] + tkn[rci + 1] + tkn[rci] + tkn[rci + 2:]\n","    return tkn\n","\n","\n","def encode_token(in_token, max_length):\n","    \"\"\"Encode token with end of sentence marker and add padding equal to max\n","    corpus token length.\n","\n","    :param in_token: input token -- str\n","    :param max_length: corpus max token length -- int\n","    :return: encoded token -- list\n","    \"\"\"\n","    in_token += EOS * (max_length - len(in_token))\n","    return [in_token]\n","\n","\n","def transform(in_tokens, max_length, noise_rate=0.8, shuffle=True):\n","    \"\"\"Transform corpus tokens into encoder inputs and decoder targets.  All\n","    tokens are padded to the length of the largest corpus token.\n","\n","    :param in_tokens: input tokens -- list\n","    :param max_length: corpus token max length -- int\n","    :param noise_rate: error rate for nous\n","    :param shuffle:\n","    :return:\n","    \"\"\"\n","    # Initialise encoded, decoded, and target token lists\n","    encoder_out_tokens, decoder_out_tokens = list(), list()\n","    target_out_tokens = list()\n","    # If set to True, the corpus tokens will be shuffled from their sorted in\n","    # alphabetical state, this will help with the model validation split on the\n","    # training data\n","    if shuffle:\n","        print('Shuffling data...')\n","        np.random.shuffle(in_tokens)\n","    # For each token in the corpus...\n","    for token in in_tokens:\n","        # Add noise to the token for the encoder\n","        encoder_token = _add_noise_to_token(token, noise_rate=noise_rate)\n","        # Pad encoded token with EOS marker (*) equal to corpus token max\n","        # length\n","        encoder_token += EOS * (max_length - len(encoder_token))\n","        # Add token to encoded tokens list\n","        encoder_out_tokens.append(encoder_token)\n","        # Add SOS marker for decoded token ($)\n","        decoder_token = SOS + token\n","        # Pad decoded token with EOS marker (*) equal to corpus token max\n","        # length\n","        decoder_token += EOS * (max_length - len(decoder_token))\n","        # Add token to decoded tokens lis\n","        decoder_out_tokens.append(decoder_token)\n","        # Remove the SOS token from the decoder to create target token\n","        target_token = decoder_token[1:]\n","        # Pad remaining space\n","        target_token += EOS * (max_length - len(target_token))\n","        # Add token to target tokens lis\n","        target_out_tokens.append(target_token)\n","        # Assert all three token variants are equal in length\n","        assert (len(encoder_token) == len(decoder_token) == len(target_token))\n","    return encoder_out_tokens, decoder_out_tokens, target_out_tokens"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1knXi-REvEri","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":103},"outputId":"723ef0be-c7bd-4720-838d-f125a6c7ed71","executionInfo":{"status":"ok","timestamp":1572192689124,"user_tz":0,"elapsed":2601,"user":{"displayName":"Michael McAleer","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBA484SnEl51AELTFepW1wp9_GgYnxoLyFNlRtVUg=s64","userId":"03089754270101753013"}}},"source":["# Transform corpus tokens into encoder, decoder, and target tokens\n","encoder_tokens, decoder_tokens, target_tokens = transform(corpus_tokens,\n","                                                          corpus_max_len)\n","\n","# Generate encoder and decoder unique character sets\n","input_chars = sorted(set(' '.join(encoder_tokens)))\n","target_chars = sorted(set(' '.join(decoder_tokens)))\n","num_encoder_tokens = len(input_chars)\n","num_decoder_tokens = len(target_chars)\n","# Calculate the max length of encoder and decoder\n","max_encoder_len = max([len(txt) for txt in encoder_tokens])\n","max_decoder_len = max([len(txt) for txt in decoder_tokens])\n","# Generate dictionary mapping for efficient character to index\n","# encoding/decoding lookups\n","input_char_to_index = dict(\n","    [(char, i) for i, char in enumerate(input_chars)])\n","target_char_to_index = dict(\n","    [(char, i) for i, char in enumerate(target_chars)])\n","# Generate dictionary mapping for efficient index to character\n","# encoding/decoding lookups\n","input_index_to_char = dict(\n","    (i, char) for char, i in input_char_to_index.items())\n","target_index_to_char = dict(\n","    (i, char) for char, i in target_char_to_index.items())\n","\n","print('Number of unique encoder characters:', num_encoder_tokens)\n","print('Number of unique decoder characters:', num_decoder_tokens)\n","print('Max token length for encoder:', max_encoder_len)\n","print('Max token length for decoder:', max_decoder_len)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Shuffling data...\n","Number of unique encoder characters: 55\n","Number of unique decoder characters: 30\n","Max token length for encoder: 23\n","Max token length for decoder: 23\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qlsLV2QDvVb6","colab_type":"code","colab":{}},"source":["# One hot encode the encoder, decoder, and target tokens\n","\n","# Generate empty numpy arrays with zeros with dimensions\n","# max_token_length x num_input_characters\n","encoder_one_hot = np.zeros(\n","    (len(encoder_tokens), max_encoder_len, num_encoder_tokens),\n","    dtype='float32')\n","decoder_one_hot = np.zeros(\n","    (len(encoder_tokens), max_decoder_len, num_decoder_tokens),\n","    dtype='float32')\n","target_one_hot = np.zeros(\n","    (len(encoder_tokens), max_decoder_len, num_decoder_tokens),\n","    dtype='float32')\n","\n","# For each token in the encoder and decoder vectors, iterate over each row of\n","# the zero-d numpy array and convert the corresponding character index to 1.\n","# This operation has been zipped into one for loop to prevent two iterative\n","# loops of full corpus length.\n","for i, (input_text, target_text) in enumerate(\n","        zip(encoder_tokens, decoder_tokens)):\n","    # One-hot encode the encoder tokens\n","    for t, char in enumerate(input_text):\n","        encoder_one_hot[i, t, input_char_to_index[char]] = 1.\n","    encoder_one_hot[i, t + 1:, input_char_to_index[' ']] = 1.\n","    # One hot encode the decoder tokens\n","    for t, char in enumerate(target_text):\n","        decoder_one_hot[i, t, target_char_to_index[char]] = 1.\n","        if t > 0:\n","            # decoder does not include the SOS marker so is not included\n","            target_one_hot[i, t - 1, target_char_to_index[char]] = 1.\n","    decoder_one_hot[i, t + 1:, target_char_to_index[' ']] = 1.\n","    target_one_hot[i, t:, target_char_to_index[' ']] = 1."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EwKxmmYOvilJ","colab_type":"text"},"source":["## Build & Fit LSTM Model on Training Data"]},{"cell_type":"code","metadata":{"id":"NFtSSqnGvoZS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"f5933b05-e9a7-425a-d4d5-845d03ecc911","executionInfo":{"status":"ok","timestamp":1572193669454,"user_tz":0,"elapsed":982891,"user":{"displayName":"Michael McAleer","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBA484SnEl51AELTFepW1wp9_GgYnxoLyFNlRtVUg=s64","userId":"03089754270101753013"}}},"source":["# Define encoder input shape\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","# Add encoder LSTM layer\n","encoder = LSTM(HIDDEN_DIM, return_state=True)\n","# Process input sequence, only the hidden and memory states need to be retained\n","__, hidden_state, memory_state = encoder(encoder_inputs)\n","# Store the encoder states for later in the model\n","encoder_states = [hidden_state, memory_state]\n","\n","# Define decoder input shape\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","# Add decoder LSTM layer, sequences are returned and states are returned for\n","# use in inference\n","decoder_lstm = LSTM(HIDDEN_DIM, return_sequences=True, return_state=True)\n","# Define the decoder using the encoder states as the initial state\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n","                                     initial_state=encoder_states)\n","# Add dense layer of softmax activation neurons equal to the amount of decoder\n","# tokens\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define mode that will take encoder inputs decoder inputs and outputs\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","# Compile the model\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","# Fit the model with a validation split of 10%\n","model.fit([encoder_one_hot, decoder_one_hot], target_one_hot,\n","          batch_size=BATCH_SIZE, epochs=N_EPOCHS, validation_split=0.1)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Train on 73832 samples, validate on 8204 samples\n","Epoch 1/70\n","73832/73832 [==============================] - 20s 273us/step - loss: 1.4552 - acc: 0.5999 - val_loss: 1.2861 - val_acc: 0.6126\n","Epoch 2/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 1.2632 - acc: 0.6177 - val_loss: 1.1952 - val_acc: 0.6215\n","Epoch 3/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 1.1539 - acc: 0.6435 - val_loss: 1.1300 - val_acc: 0.6450\n","Epoch 4/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 1.0763 - acc: 0.6603 - val_loss: 1.0504 - val_acc: 0.6527\n","Epoch 5/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 1.0309 - acc: 0.6658 - val_loss: 1.0273 - val_acc: 0.6639\n","Epoch 6/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.9889 - acc: 0.6835 - val_loss: 0.9900 - val_acc: 0.6691\n","Epoch 7/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.9536 - acc: 0.6868 - val_loss: 0.9222 - val_acc: 0.7088\n","Epoch 8/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.9117 - acc: 0.6992 - val_loss: 0.8641 - val_acc: 0.7389\n","Epoch 9/70\n","73832/73832 [==============================] - 14s 186us/step - loss: 0.8606 - acc: 0.7216 - val_loss: 0.9081 - val_acc: 0.6958\n","Epoch 10/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.8090 - acc: 0.7389 - val_loss: 0.7908 - val_acc: 0.7602\n","Epoch 11/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.7524 - acc: 0.7570 - val_loss: 0.7549 - val_acc: 0.7453\n","Epoch 12/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.7030 - acc: 0.7782 - val_loss: 0.6717 - val_acc: 0.7970\n","Epoch 13/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.6532 - acc: 0.7936 - val_loss: 0.6331 - val_acc: 0.7932\n","Epoch 14/70\n","73832/73832 [==============================] - 14s 186us/step - loss: 0.6042 - acc: 0.8110 - val_loss: 0.5536 - val_acc: 0.8379\n","Epoch 15/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.5642 - acc: 0.8229 - val_loss: 0.5497 - val_acc: 0.8323\n","Epoch 16/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.5252 - acc: 0.8391 - val_loss: 0.5496 - val_acc: 0.8105\n","Epoch 17/70\n","73832/73832 [==============================] - 14s 188us/step - loss: 0.4865 - acc: 0.8496 - val_loss: 0.4718 - val_acc: 0.8626\n","Epoch 18/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.4463 - acc: 0.8646 - val_loss: 0.4217 - val_acc: 0.8732\n","Epoch 19/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.4181 - acc: 0.8753 - val_loss: 0.3927 - val_acc: 0.8838\n","Epoch 20/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.3758 - acc: 0.8887 - val_loss: 0.3330 - val_acc: 0.9042\n","Epoch 21/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.3541 - acc: 0.8962 - val_loss: 0.5574 - val_acc: 0.8348\n","Epoch 22/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.3348 - acc: 0.9024 - val_loss: 0.3062 - val_acc: 0.9104\n","Epoch 23/70\n","73832/73832 [==============================] - 14s 189us/step - loss: 0.3098 - acc: 0.9105 - val_loss: 0.2788 - val_acc: 0.9197\n","Epoch 24/70\n","73832/73832 [==============================] - 14s 189us/step - loss: 0.2907 - acc: 0.9178 - val_loss: 0.2826 - val_acc: 0.9171\n","Epoch 25/70\n","73832/73832 [==============================] - 14s 188us/step - loss: 0.2818 - acc: 0.9206 - val_loss: 0.2606 - val_acc: 0.9270\n","Epoch 26/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.2654 - acc: 0.9255 - val_loss: 0.2352 - val_acc: 0.9354\n","Epoch 27/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.2386 - acc: 0.9342 - val_loss: 0.2227 - val_acc: 0.9392\n","Epoch 28/70\n","73832/73832 [==============================] - 14s 188us/step - loss: 0.2422 - acc: 0.9333 - val_loss: 0.2340 - val_acc: 0.9334\n","Epoch 29/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.2256 - acc: 0.9391 - val_loss: 0.3964 - val_acc: 0.8777\n","Epoch 30/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.2070 - acc: 0.9444 - val_loss: 0.2074 - val_acc: 0.9431\n","Epoch 31/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.2097 - acc: 0.9440 - val_loss: 0.2129 - val_acc: 0.9411\n","Epoch 32/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.1924 - acc: 0.9487 - val_loss: 0.5418 - val_acc: 0.9067\n","Epoch 33/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.1948 - acc: 0.9500 - val_loss: 0.2108 - val_acc: 0.9430\n","Epoch 34/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.1780 - acc: 0.9542 - val_loss: 0.1859 - val_acc: 0.9498\n","Epoch 35/70\n","73832/73832 [==============================] - 14s 189us/step - loss: 0.1708 - acc: 0.9561 - val_loss: 0.1735 - val_acc: 0.9535\n","Epoch 36/70\n","73832/73832 [==============================] - 14s 190us/step - loss: 0.1717 - acc: 0.9570 - val_loss: 0.3576 - val_acc: 0.9022\n","Epoch 37/70\n","73832/73832 [==============================] - 14s 190us/step - loss: 0.1663 - acc: 0.9581 - val_loss: 0.1651 - val_acc: 0.9566\n","Epoch 38/70\n","73832/73832 [==============================] - 14s 189us/step - loss: 0.1528 - acc: 0.9614 - val_loss: 0.1526 - val_acc: 0.9612\n","Epoch 39/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.1515 - acc: 0.9628 - val_loss: 0.1718 - val_acc: 0.9529\n","Epoch 40/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.1474 - acc: 0.9640 - val_loss: 0.1694 - val_acc: 0.9529\n","Epoch 41/70\n","73832/73832 [==============================] - 14s 188us/step - loss: 0.1431 - acc: 0.9653 - val_loss: 0.1610 - val_acc: 0.9563\n","Epoch 42/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.1370 - acc: 0.9669 - val_loss: 0.1522 - val_acc: 0.9602\n","Epoch 43/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.1351 - acc: 0.9682 - val_loss: 0.1554 - val_acc: 0.9603\n","Epoch 44/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.1287 - acc: 0.9693 - val_loss: 0.1419 - val_acc: 0.9643\n","Epoch 45/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.1262 - acc: 0.9704 - val_loss: 0.1470 - val_acc: 0.9620\n","Epoch 46/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.1243 - acc: 0.9709 - val_loss: 0.1365 - val_acc: 0.9647\n","Epoch 47/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.1211 - acc: 0.9705 - val_loss: 0.1301 - val_acc: 0.9667\n","Epoch 48/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.1170 - acc: 0.9728 - val_loss: 0.1412 - val_acc: 0.9627\n","Epoch 49/70\n","73832/73832 [==============================] - 14s 188us/step - loss: 0.1166 - acc: 0.9723 - val_loss: 0.1263 - val_acc: 0.9680\n","Epoch 50/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.1095 - acc: 0.9745 - val_loss: 0.1418 - val_acc: 0.9624\n","Epoch 51/70\n","73832/73832 [==============================] - 14s 188us/step - loss: 0.1169 - acc: 0.9729 - val_loss: 0.1321 - val_acc: 0.9660\n","Epoch 52/70\n","73832/73832 [==============================] - 14s 188us/step - loss: 0.1101 - acc: 0.9742 - val_loss: 0.1195 - val_acc: 0.9704\n","Epoch 53/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.1077 - acc: 0.9756 - val_loss: 0.1214 - val_acc: 0.9698\n","Epoch 54/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.1076 - acc: 0.9747 - val_loss: 0.1297 - val_acc: 0.9666\n","Epoch 55/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.1033 - acc: 0.9764 - val_loss: 0.1169 - val_acc: 0.9711\n","Epoch 56/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.0999 - acc: 0.9760 - val_loss: 0.1182 - val_acc: 0.9706\n","Epoch 57/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.0962 - acc: 0.9782 - val_loss: 0.1228 - val_acc: 0.9690\n","Epoch 58/70\n","73832/73832 [==============================] - 14s 188us/step - loss: 0.0957 - acc: 0.9783 - val_loss: 0.1193 - val_acc: 0.9700\n","Epoch 59/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.0969 - acc: 0.9776 - val_loss: 0.1195 - val_acc: 0.9702\n","Epoch 60/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.0943 - acc: 0.9783 - val_loss: 0.1175 - val_acc: 0.9699\n","Epoch 61/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.0929 - acc: 0.9785 - val_loss: 0.1124 - val_acc: 0.9728\n","Epoch 62/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.0906 - acc: 0.9792 - val_loss: 0.1195 - val_acc: 0.9701\n","Epoch 63/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.0855 - acc: 0.9808 - val_loss: 0.1172 - val_acc: 0.9709\n","Epoch 64/70\n","73832/73832 [==============================] - 14s 188us/step - loss: 0.0883 - acc: 0.9801 - val_loss: 0.1197 - val_acc: 0.9702\n","Epoch 65/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.0835 - acc: 0.9808 - val_loss: 0.1265 - val_acc: 0.9677\n","Epoch 66/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.0811 - acc: 0.9815 - val_loss: 0.1063 - val_acc: 0.9742\n","Epoch 67/70\n","73832/73832 [==============================] - 14s 188us/step - loss: 0.0819 - acc: 0.9818 - val_loss: 0.1106 - val_acc: 0.9728\n","Epoch 68/70\n","73832/73832 [==============================] - 14s 187us/step - loss: 0.0812 - acc: 0.9819 - val_loss: 0.1097 - val_acc: 0.9740\n","Epoch 69/70\n","73832/73832 [==============================] - 14s 188us/step - loss: 0.0816 - acc: 0.9814 - val_loss: 0.1109 - val_acc: 0.9729\n","Epoch 70/70\n","73832/73832 [==============================] - 14s 188us/step - loss: 0.0782 - acc: 0.9819 - val_loss: 0.1092 - val_acc: 0.9734\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f34c6e35da0>"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"Dn3LLtWVvvIS","colab_type":"code","colab":{}},"source":["# With model trained on corpus the model encoder and decoder can be defined\n","# for use in character level spelling sequence prediction on unseen data\n","\n","# Extract the encoder using the inputs and states defined previously\n","encoder_model = Model(encoder_inputs, encoder_states)\n","# Define decoder hidden and memory state inputs\n","decoder_state_input_hidden = Input(shape=(HIDDEN_DIM,))\n","decoder_state_input_memory = Input(shape=(HIDDEN_DIM,))\n","# Combine decoder input states\n","decoder_states_inputs = [decoder_state_input_hidden,\n","                         decoder_state_input_memory]\n","# Extract the decoder\n","decoder_outputs, hidden_state, memory_state = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs)\n","# Set decoder states\n","decoder_states = [hidden_state, memory_state]\n","# Recreate our dense output layer for decoder outputs\n","decoder_outputs = decoder_dense(decoder_outputs)\n","# Combine decoder input and output states\n","decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n","                      [decoder_outputs] + decoder_states)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"62ske2T9wV3U","colab_type":"text"},"source":["## Decode Model Predictions"]},{"cell_type":"code","metadata":{"id":"xeh7_RM5wZg0","colab_type":"code","colab":{}},"source":["def decode_sequence(input_seq, e_model, d_model, d_chars,\n","                    tgt_index_char_map):\n","    \"\"\"Decode an input sequence to get a predicted output sequence.\n","\n","    :param input_seq: encoded input sequence -- np.array\n","    :param e_model: encoder model -- keras model\n","    :param d_model: decoder model -- keras model\n","    :param d_chars: number of decoder characters -- int\n","    :param tgt_index_char_map: target index to character map -- dict\n","    :return: decoded string -- str\n","    \"\"\"\n","    # Initialise decoded string response\n","    decoded_string = ''\n","    # Get encoded sequence state after going through encoder, used in decoding\n","    states_value = e_model.predict(input_seq)\n","    # Generate empty target sequence of dimension 1 x number of decoder chars\n","    target_seq = np.zeros((1, 1, d_chars))\n","    # Set start of target sequence char as start of sequence marker\n","    target_seq[0, 0, target_char_to_index[SOS]] = 1.\n","    # Set continuous loop until stop condition has been met\n","    stop_condition = False\n","    while not stop_condition:\n","        # Predict the next character probabilities from the decoder using the\n","        # state of the sequence after going through the encoder\n","        output_tokens, hidden, memory = d_model.predict(\n","            [target_seq] + states_value)\n","        # From the character probabilities, take the character index with the\n","        # highest probability\n","        predicted_token_index = np.argmax(output_tokens[0, -1, :])\n","        # Get the corresponding character from the target index to char map\n","        predicted_char = tgt_index_char_map[predicted_token_index]\n","        # Add the character to the decoded string\n","        decoded_string += predicted_char\n","        # If the predicted character was the end of string marker or the\n","        # decoder has hit the maximum decoder token length, stop the loop\n","        if predicted_char == EOS or len(decoded_string) > max_decoder_len:\n","            stop_condition = True\n","        # Update the target sequence with the predicted character\n","        target_seq = np.zeros((1, 1, d_chars))\n","        target_seq[0, 0, predicted_token_index] = 1.\n","        # Update states\n","        states_value = [hidden, memory]\n","\n","    return decoded_string\n","\n","\n","def predict_spelling(token, e_model, d_model, c_max_len, e_max_len, e_chars,\n","                     d_chars, input_c_to_i, tgt_i_to_c):\n","    \"\"\"Predict the correct spelling of an input string using LSTM model.\n","\n","    :param token: input token for error correction -- str\n","    :param e_model: encoder model -- keras model\n","    :param d_model: decoder model -- keras model\n","    :param c_max_len: corpus max length -- int\n","    :param e_max_len: encoder max length -- int\n","    :param e_chars: number of encoder characters -- int\n","    :param d_chars: number of decoder characters -- int\n","    :param input_c_to_i: input character to index map -- dict\n","    :param tgt_i_to_c: input index to character map -- dict\n","    :return: predicted correct spelling -- str\n","    \"\"\"\n","    # Encode the input string\n","    encoded_incorrect_token = encode_token(token, c_max_len)\n","    # Create an empty numpy array of zeros of same dimensions as training data\n","    encoder_incorrect_data = np.zeros((1, e_max_len, e_chars), dtype='float32')\n","    # One hot encode the input string\n","    for i, encoded_incorrect_token in enumerate(encoded_incorrect_token):\n","        for x, char in enumerate(encoded_incorrect_token):\n","            encoder_incorrect_data[i, x, input_c_to_i[char]] = 1.\n","    # Predict the spelling of the input string\n","    decoded_sentence = decode_sequence(\n","        encoder_incorrect_data, e_model, d_model, d_chars, tgt_i_to_c)\n","    # Remove any EOS markers in the string after decoding\n","    decoded_sentence = re.sub('\\*', '', decoded_sentence)\n","    return decoded_sentence"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BeXs-Mw9wotl","colab_type":"code","colab":{}},"source":["def run_spell_checker(incorrect_word_list, e_model, d_model, c_set, c_max_len, \n","                      e_max_len, e_chars, d_chars, input_c_to_i, tgt_i_to_c, \n","                      predict_cnt=5):\n","    \"\"\"Run the LSTM char-to-char spell checker on a list of incorrectly spelled\n","    words.\n","    \n","    :param incorrect_word_list: incorrect words -- list\n","    :param e_model: encoder model -- keras model\n","    :param d_model: decoder model -- keras model\n","    :param c_set: clean corpus tokens -- set\n","    :param c_max_len: corpus max length -- int\n","    :param e_max_len: encoder max length -- int\n","    :param e_chars: number of encoder characters -- int\n","    :param d_chars: number of decoder characters -- int\n","    :param input_c_to_i: input character to index map -- dict\n","    :param tgt_i_to_c: input index to character map -- dict\n","    :param predict_cnt: amount of additional predictions to try -- int\n","    :return: english word possibilities -- dict\n","    \"\"\"\n","    # Create response dict to hold predicted possibilities of correct spelling\n","    response = dict()\n","    # For each of the incorrectly spelled words from the OCR output\n","    for word in incorrect_word_list:\n","        # Add word as a key in the response dict with a list value to hold\n","        # spelling possibilities\n","        response[word] = list()\n","        # Predict the spelling of the incorrect word\n","        spelling = predict_spelling(\n","            word, e_model, d_model, c_max_len, e_max_len, e_chars, d_chars, \n","            input_c_to_i, tgt_i_to_c)\n","        # If the predicted spelling is in the corpus add it to the response \n","        # dict in the corresponding key list\n","        if spelling in c_set:\n","            response[word].append(spelling)\n","        # For completeness, predict a range of additional possibilities by\n","        # adding noise to the incorrect word and predicting again\n","        inc_tok = [word] * predict_cnt\n","        incorrect_tokens, decoded_tokens, target_tokens = transform(\n","            inc_tok, corpus_max_len, shuffle=False)\n","        for token in incorrect_tokens:\n","            alternative_spelling = predict_spelling(\n","                token, e_model, d_model, c_max_len, e_max_len, e_chars,\n","                d_chars, input_c_to_i, tgt_i_to_c)\n","            # If the noisy alternative prediction is in the corpus add it to\n","            # the response dict\n","            if alternative_spelling in c_set:\n","                response[word].append(alternative_spelling)\n","        # After all predictions are complete, filter out any repeats and\n","        # predictions which may match the incorrect spelling\n","        response[word] = set([i for i in response[word] if i != word])\n","    return response"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8b2IV3rtyImd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3f6e8922-eb03-4a4b-8f07-b28b621d2c28","executionInfo":{"status":"ok","timestamp":1572196419364,"user_tz":0,"elapsed":6185,"user":{"displayName":"Michael McAleer","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBA484SnEl51AELTFepW1wp9_GgYnxoLyFNlRtVUg=s64","userId":"03089754270101753013"}}},"source":["predicted_spellings = run_spell_checker(\n","    incorrect_words, encoder_model, decoder_model, corpus_lookup_set,\n","    corpus_max_len, max_encoder_len, num_encoder_tokens, num_decoder_tokens,\n","    input_char_to_index, target_index_to_char, predict_cnt=50)\n","print(predicted_spellings)"],"execution_count":67,"outputs":[{"output_type":"stream","text":["{'goinng': {'going'}, 'thnk': {'tank'}, 'dnner': {'dinner', 'den'}}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JHWuIs2UyX3s","colab_type":"text"},"source":["# Task 3.2 Calculate Levenshtein Distance"]},{"cell_type":"code","metadata":{"id":"5WxW0jSezjrc","colab_type":"code","colab":{}},"source":["def calculate_levenshtein_distance(seq_a, seq_b):\n","    \"\"\"Calculate levenshtein distance between two input strings.\n","    \n","    :param seq_a: input string -- str\n","    :param seq_b: input string -- str\n","    :return: distance -- int\n","    \"\"\"\n","    # Set the size of the matrix\n","    size_x, size_y = len(seq_a) + 1, len(seq_b) + 1\n","    # Initialise matrix\n","    m = np.zeros((size_x, size_y))\n","    # Set row/col integer labels\n","    for x in range(size_x):\n","        m[x, 0] = x\n","    for y in range(size_y):\n","        m[0, y] = y\n","    # For each [x, y] position, compare row-wise and column wise\n","    for x in range(1, size_x):\n","        for y in range(1, size_y):\n","            # If two letters are equal, the new value at position [x, y]\n","            # is the minimum between the value of position [x-1, y] + 1,\n","            # position [x-1, y-1], and position [x, y-1] + 1.\n","            if seq_a[x - 1] == seq_b[y - 1]:\n","                m[x, y] = min(m[x - 1, y] + 1, m[x - 1, y - 1],\n","                              m[x, y - 1] + 1)\n","            # Else it is the minimum between the value of\n","            # position [x-1, y] + 1, position [x-1, y-1] + 1, and\n","            # position [x, y-1] + 1\n","            else:\n","                m[x, y] = min(m[x - 1, y] + 1, m[x - 1, y - 1] + 1,\n","                              m[x, y - 1] + 1)\n","    # Return the difference in the two strings minus the row and col labels\n","    return m[size_x - 1, size_y - 1]\n","\n","\n","def calculate_shortest_distances(predictions):\n","    \"\"\"Calculate the shortest distance between an incorrect word and all\n","    possible correct words.\n","    \n","    :param predictions: model predictions -- dict\n","    :return: short distance info, all distance info -- tuple(dict, dict)\n","    \"\"\"\n","    # Initialise response dicts\n","    all_distance = dict()\n","    short_distance = dict()\n","    # For each of the incorrectly spelled words\n","    for word_key in predictions.keys():\n","        # Set the initial distance as infinite\n","        shortest_distance = (None, float('inf'))\n","        # Initialise all distances list to hold all possibility info\n","        all_distance[word_key] = list()\n","        # For each of the correct spelling predictions\n","        for alternative in predictions[word_key]:\n","            # Calculate the distance between the incorrect word and the \n","            # predicted variant\n","            distance = calculate_levenshtein_distance(word_key, alternative)\n","            # Create distance info tuple to hold prediction and distance\n","            distance_tuple = (alternative, distance)\n","            # Add the distance info to the all distance dict\n","            all_distance[word_key].append(distance_tuple)\n","            # If the calculated distance is the lowest distance encountered\n","            if distance < shortest_distance[1]:\n","                # Set the current distance as the shortest distance\n","                shortest_distance = (alternative, distance)\n","                short_distance[word_key] = shortest_distance\n","\n","    return short_distance, all_distance"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0crCv6MOzlOq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":86},"outputId":"d6841b90-1394-4f1f-ac55-7b43e7305f8b","executionInfo":{"status":"ok","timestamp":1572196484325,"user_tz":0,"elapsed":1015,"user":{"displayName":"Michael McAleer","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBA484SnEl51AELTFepW1wp9_GgYnxoLyFNlRtVUg=s64","userId":"03089754270101753013"}}},"source":["short_distances, all_distances = calculate_shortest_distances(\n","    predicted_spellings)\n","print('Shortest distance predictions:')\n","for k, v in short_distances.items():\n","    print('--\"{k}\" shortest distance word: {v}'.format(k=k, v=v))"],"execution_count":72,"outputs":[{"output_type":"stream","text":["Shortest distance predictions:\n","--\"goinng\" shortest distance word: ('going', 1.0)\n","--\"thnk\" shortest distance word: ('tank', 1.0)\n","--\"dnner\" shortest distance word: ('dinner', 1.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_TxQZ6HPNRgz","colab_type":"text"},"source":["# Apply Change to OCR Prediction and Re-Contract"]},{"cell_type":"code","metadata":{"id":"MotAep9vMmqt","colab_type":"code","colab":{}},"source":["def apply_spelling_change(closest_distances, in_tokens):\n","    \"\"\"Apply the closest distance token to the original OCR output sentence\n","    and re-contract.\n","    \n","    :param closest_distances: shortest distance info -- dict\n","    :param in_tokens: OCR output tokens -- list\n","    :return: modified re-contracted string -- list\n","    \"\"\"\n","    response_tokens = deepcopy(in_tokens)\n","    for x in closest_distances.keys():\n","        correction = closest_distances[x]\n","        for i, w in enumerate(ocr_tokens):\n","            if w == x:\n","                response_tokens[i] = correction[0]\n","    correct_output = ' '.join(response_tokens)\n","    return list((py_cont.contract_texts([correct_output])))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dADWJoNn7USd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"b880365e-b82f-4f7d-d66d-b2f2d8fb3764","executionInfo":{"status":"ok","timestamp":1572196496170,"user_tz":0,"elapsed":1151,"user":{"displayName":"Michael McAleer","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBA484SnEl51AELTFepW1wp9_GgYnxoLyFNlRtVUg=s64","userId":"03089754270101753013"}}},"source":["corrected_ocr_tokens = apply_spelling_change(short_distances, ocr_tokens)\n","print('Original OCR Output: {o}'.format(o=ocr_tokens))\n","print('Error module corrected output: {o}'.format(o=corrected_ocr_tokens))"],"execution_count":74,"outputs":[{"output_type":"stream","text":["Original OCR Output: ['we', 'are', 'goinng', 'to', 'the', 'zoo', 'and', 'I', 'do', 'not', 'thnk', 'I', 'will', 'be', 'home', 'for', 'dnner']\n","Error module corrected output: [\"we're going to the zoo and I don't tank I'll be home for dinner\"]\n"],"name":"stdout"}]}]}