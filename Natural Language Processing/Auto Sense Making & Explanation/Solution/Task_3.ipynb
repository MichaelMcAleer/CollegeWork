{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task-3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPo4mE9Hcumg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -----------------------------------------------------\n",
        "# Natural Language Processing\n",
        "# Assignment 2 - Automatic Sense Making and Explanation\n",
        "# Task 3 - Explanation (Generation)\n",
        "# Michael McAleer R00143621\n",
        "# -----------------------------------------------------\n",
        "# Notes: \n",
        "# 1. This has been run and tested on Python 3.6 with TensorFlow 1.15.0\n",
        "# 2. This model only predicts on 100 samples from test data set due to time \n",
        "#    involved in predicting non-sensical answers\n",
        "# 3. This model was only tested with 10 training epochs because Google\n",
        "#    restricted GPU usage due to unfair usage or resources - it takes a\n",
        "#    considerable amount of time to train per epoch (5-7mins)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKNaDuIZr5c4",
        "colab_type": "text"
      },
      "source": [
        "### 1. Install package dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpXolBw34j7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q textgenrnn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BLAvCQVr-o_",
        "colab_type": "text"
      },
      "source": [
        "### 2. Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGzQO__f4nAJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "e6b60a5c-15ec-4247-825a-508475a35058"
      },
      "source": [
        "import argparse\n",
        "import collections\n",
        "import csv\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from textgenrnn import textgenrnn\n",
        "from typing import List, Dict"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D7qaJZUsFJo",
        "colab_type": "text"
      },
      "source": [
        "### 3. Set paths, download dependencies, set constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyYWEvlc4pW2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad531c47-6e6f-4b30-a935-64e43f1e3c90"
      },
      "source": [
        "# Set root path dependent on system\n",
        "# If System is Windows, set ROOT_DIR as current working directory\n",
        "if os.name == 'nt':\n",
        "    ROOT_DIR = os.getcwd()\n",
        "# Else running on CoLab, set ROOT_DIR to match environment path\n",
        "else:\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount('/content/drive')\n",
        "    ROOT_DIR = '/content/drive/My Drive/Colab Notebooks'\n",
        "\n",
        "# Paths to data and model output dir\n",
        "DATA_DIR = '{root}/data'.format(root=ROOT_DIR)\n",
        "TRAIN_DIR = '{data}/train'.format(data=DATA_DIR)\n",
        "TEST_DIR = '{data}/test'.format(data=DATA_DIR)\n",
        "\n",
        "# Evaluator constants\n",
        "EXIT_STATUS_ANSWERS_MALFORMED = 1\n",
        "EXIT_STATUS_PREDICTIONS_MALFORMED = 2\n",
        "EXIT_STATUS_PREDICTIONS_EXTRA = 3\n",
        "EXIT_STATUS_PREDICTION_MISSING = 4"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehkQwGYbsHS4",
        "colab_type": "text"
      },
      "source": [
        "### 4. Import training and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1NXK3324sIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_csv_values(file_name, answers_file=False):\n",
        "    \"\"\"Read any csv file.\n",
        "\n",
        "    :param file_name: path to CSV file -- str\n",
        "    :param answers_file: if the CSV file contains dataset answers -- bool\n",
        "    :return: CSV parsed data -- dict\n",
        "    \"\"\"\n",
        "    # open the file in universal line ending mode\n",
        "    with open(file_name, newline='') as infile:\n",
        "        # if the file contains answers set the column names manually\n",
        "        if answers_file:\n",
        "            reader = csv.DictReader(infile, fieldnames=['id', 'answer0',\n",
        "                                                        'answer1', 'answer2'])\n",
        "        else:\n",
        "            # read the file as a list for each row ({header : value})\n",
        "            reader = csv.DictReader(infile)\n",
        "        # Initialise response dict\n",
        "        data = dict()\n",
        "        # For each row in the file\n",
        "        for row in reader:\n",
        "            # For header, value in each row\n",
        "            for header, value in row.items():\n",
        "                try:\n",
        "                    # Add the value to the existing list\n",
        "                    data[header].append(value)\n",
        "                except KeyError:\n",
        "                    # Create a new list for the header and assign value\n",
        "                    data[header] = [value]\n",
        "    return data\n",
        "\n",
        "\n",
        "# Set training data path\n",
        "training_data_path = '{td}/subtaskC_data_all.csv'.format(td=TRAIN_DIR)\n",
        "training_answers_path = '{td}/subtaskC_answers_all.csv'.format(td=TRAIN_DIR)\n",
        "# Load training data\n",
        "train_data = read_csv_values(training_data_path)\n",
        "train_answers = read_csv_values(training_answers_path, answers_file=True)\n",
        "\n",
        "# Set test data path\n",
        "test_data_path = '{td}/taskC_trial_data.csv'.format(td=TEST_DIR)\n",
        "test_answers_path = '{td}/taskC_trial_references.csv'.format(td=TEST_DIR)\n",
        "# Load test data\n",
        "test_data = read_csv_values(test_data_path)\n",
        "test_answers = read_csv_values(test_answers_path, answers_file=True)\n",
        "\n",
        "# Initialise corpus list\n",
        "corpus = list()\n",
        "# For each context and possible reference answer\n",
        "for c, a0, a1, a2 in zip(train_data['FalseSent'], train_answers['answer0'],\n",
        "                         train_answers['answer1'], train_answers['answer2']):\n",
        "    # For each answer\n",
        "    for a in [a0, a1, a2]:\n",
        "        # If the answer doesn't end in period add one, this will help\n",
        "        # the model determine that a new sentence is required and not a\n",
        "        # continuation of the context\n",
        "        if c[-1] != '.':\n",
        "            c += '.'\n",
        "        # Add the context and answer the corpus\n",
        "        corpus.append('{context} {answer}'.format(context=c, answer=a))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrQ8IdALsMQJ",
        "colab_type": "text"
      },
      "source": [
        "### 5. Transfer & Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxV8oRL24wEx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ada9b604-15de-4de2-c929-b402a357b41a"
      },
      "source": [
        "# Initialise Char-RNN (https://github.com/karpathy/char-rnn) module\n",
        "# TextGenRNN (https://pypi.org/project/textgenrnn/)\n",
        "\n",
        "# The model is pre-trained on a Reddit corpus and consists of multiple LSTM\n",
        "# layers with attention included that determines the next character in a\n",
        "# sequence of 394 possible characters. Retraining of the model is done with\n",
        "# a momentum based optimiser and linearly decaying learning rate.\n",
        "textgen = textgenrnn()\n",
        "\n",
        "# The number of epochs had to be dropped significantly after getting my Colab\n",
        "# account GPU restricted, however, given the nonsensical generated text that\n",
        "# was produced, even after a number of epochs dropping the number of epochs\n",
        "# will not have a lot of impact on the quality of the results without further\n",
        "# work on the training corpus:\n",
        "# https://research.google.com/colaboratory/faq.html#gpu-availability\n",
        "textgen.train_on_texts(corpus, num_epochs=10, batch_size=1024,\n",
        "                       train_size=0.95, validation=True, dropout=0.3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Training on 2,393,457 character sequences.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/10\n",
            "2337/2337 [==============================] - 299s 128ms/step - loss: 1.2450 - val_loss: 1.1864\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "He put a bath in the sky. A dog is not a protect to see the store.\n",
            "\n",
            "He was a liquid on the moon. A shower is not a computer to complain and it is a place to cook in the sun. The moon is not a place to be pretty the sun to be eaten.\n",
            "\n",
            "He was a shopping and he was always a football. A person cannot be protected in a park.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "The people can be suddented in the sun. A computer is a place to see it for human bears. It is a space of the parents and cannot be drunk\n",
            "\n",
            "He was eating contains in a store. You cannot go out of a body in the bathroom.\n",
            "\n",
            "Cars are so hoping the rain. A bath is not real.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "you won't excrean from living food. the lady too much person cannot fall\n",
            "\n",
            "I sat out of the cere-man. CHAT for each others invisible stars\n",
            "\n",
            "She drives into that blackboak. Nimber go do not hang.\n",
            "\n",
            "Epoch 2/10\n",
            "2337/2337 [==============================] - 292s 125ms/step - loss: 1.1568 - val_loss: 1.1475\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "The store is a person to eat a park. A computer can not be played in a computer.\n",
            "\n",
            "Most people can be put on the sun. A person cannot be used to see the sun\n",
            "\n",
            "I went to the sun to the store. A car is not a computer in the sun.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "He wore a planet to the water. Heater is not a fire in the sky.\n",
            "\n",
            "The soup is a book of the computer to the hot attack. The computer can not be put on a car\n",
            "\n",
            "I saw a prize to drink so I went for a hand. Huge is not a human that it is not a place to space.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "sweet rabbits is millergates flying a metrol. People do not write dirty\n",
            "\n",
            "Jack are two garkets on the dinosaur. A pet does not dumpbase la\n",
            "\n",
            "Veganite is 6 years ago. Shoes are not eating at 8 peoples\n",
            "\n",
            "Epoch 3/10\n",
            "2337/2337 [==============================] - 292s 125ms/step - loss: 1.1206 - val_loss: 1.1274\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "The sun is a stone. The soup is not a place to stay in the sky.\n",
            "\n",
            "The cat is the late to see the train. A car is not a place to see the sea.\n",
            "\n",
            "He was so hot to stay in the sky. A cat is a common animal\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "The skin ate the comfortation. A chicken ate a countries and they are not a liquid.\n",
            "\n",
            "The stove is a blood. hairdry cannot be a gloves\n",
            "\n",
            "The baby contains money. A train cannot be used to play a chicken of the park.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Jello can buy waiter. papers are not showing application.\n",
            "\n",
            "Magic can fly in the world. Water is one, not supports game\n",
            "\n",
            "Tiping replace me tis general. Vegetables match\n",
            "\n",
            "Epoch 4/10\n",
            "2337/2337 [==============================] - 292s 125ms/step - loss: 1.0941 - val_loss: 1.1073\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "The sun is the leaves to see the sun. The police is not a place to see a banana.\n",
            "\n",
            "I washed his car to the sun. A baseball can only swim in the sky.\n",
            "\n",
            "The sun is the earth of the sky. The sun is a place to see a basketball in the sky. A car is not a place to see a ball of the sun.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "She is a fingerprizer of the temperature of the street early is a doctor.\n",
            "\n",
            "The towel is a lotion to have a bath. A person would not fit in a car on the moon.\n",
            "\n",
            "My cat is made of pair. A person would not be eaten to be a person to be a flower.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Some borkly went ham in a car on the path. Wood pieces cables asleep\n",
            "\n",
            "The yerdone ate shark. A Dog does not exist things.\n",
            "\n",
            "It is riding on better criminal dusk. fires rock the sea, and they are no necessary clothes for.\n",
            "\n",
            "Epoch 5/10\n",
            "2337/2337 [==============================] - 291s 125ms/step - loss: 1.0720 - val_loss: 1.0928\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "The sun is the most person. Starve is not a place to see the store to the tree. The sun is too heavy to be a big thing to see the sun. The stone is not a place to cook the water.\n",
            "\n",
            "The mountain is a place to see the store. Stone is not a place to see the car to hear the car. A person cannot be a liquid that cannot be driven.\n",
            "\n",
            "She was a train to watch a bath. The sun is not a place to stay a car to the store.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "He went to the tree to be warmed the sun. The car is a person to walk to the banana. There are no headphones on the room.\n",
            "\n",
            "I went to the most perspective that is used to see anything\n",
            "\n",
            "He can travel a rock. A cat is a place to see the light that a sport. The earth is not a place to pressed the patient not helpful to cook.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "The drink for travell on friends. Pilus dollars don't eat.\n",
            "\n",
            "All the card lived in the library. Centuries usually drive off bene\n",
            "\n",
            "A toilet can be seedied abround. Students can not play with it to keep surface.\n",
            "\n",
            "Epoch 6/10\n",
            "2337/2337 [==============================] - 296s 126ms/step - loss: 1.0523 - val_loss: 1.0799\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "The sun is a place to stay in the sky. Cars are not a place to stay in the sky.\n",
            "\n",
            "The sun is a place to stay in the sky. A car is not a place to stay in a store.\n",
            "\n",
            "The sun is the same thirst to see the water. A baby cannot fit in a bathtub.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "You can only die if you want to stay in the sun.\n",
            "\n",
            "She took the door in a bath in the sky. A garden is not a place to protect the earth.\n",
            "\n",
            "The store is a thing to store flowers. A lake is solid and cannot be drinking a dress.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "He must make a teacher when you were eating doesn't help my cookie, we lie.\n",
            "\n",
            "I can seo if it still gives math of my elephant forms of the floor happine. Nobody can't fly food\n",
            "\n",
            "Air and laughed by bought a cable because it's in the naked and requires army.\n",
            "\n",
            "Epoch 7/10\n",
            "2337/2337 [==============================] - 297s 127ms/step - loss: 1.0342 - val_loss: 1.0669\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "He was so hard to get some glass of the sea. The sun is not a place to stay in the sky.\n",
            "\n",
            "The plane is a place to see a bread on the sun.\n",
            "\n",
            "The computer is a place to stay in the sky. A book is not a place to save it to the sun\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "She put a tree in the sky. A book is not a color of a computer to have a breakfast that moves. people increase the sunscreen of the sun. A computer is a place where we cannot be put in a lot of controller\n",
            "\n",
            "The teacher is the most powerful material. The sun is a person who does not have the sun\n",
            "\n",
            "A person wants to be in a day of something. If you want to eat someone to see the door with a south pole. A toothbrush is not a place to study the time.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "employers are very washled by causes her students or it. your song is a musical differ city\n",
            "\n",
            "My sister is toyotal zoos in the sea. Canting sea is made opening bookthwrricling eggs. Medicine doesn't rotate between the sea\n",
            "\n",
            "Calories will write in forest to your head, so I just work so he jumped on the decay out of equal by window. Nails may pet instead of lots of causes mummilion to develop wet. Roses breeze it fulfitly the thirst girls and can't cut only\n",
            "\n",
            "Epoch 8/10\n",
            "2337/2337 [==============================] - 293s 126ms/step - loss: 1.0172 - val_loss: 1.0559\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "The stone is the same time to be able to see the sun.\n",
            "\n",
            "The teacher was so short to the temperature of the store. The sun is too big to be a place to see the sun.\n",
            "\n",
            "He was so hot to see a book with a book. A basketball is not a place to stay in the sky.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "Mineral is a small fish to see an apple to watch movies.\n",
            "\n",
            "A stone is a place to see the teacher to the ground. No one will cause a lion in the sea.\n",
            "\n",
            "You can buy a blanket to catch the cat. A car is so artificial comfortable distances and animals. Cats don't fly.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "A gun is sad as a controlace of an outdoor. All hissely people trust the god even they are solar on  tasks\n",
            "\n",
            "When express you with his moisture. waterburds are not out fuel to faste and can not age the sideway\n",
            "\n",
            "Watchina has interesting food. Bike sensitive running to control the zoo cable. The plather is drinking kids live the white way to the teeth.\n",
            "\n",
            "Epoch 9/10\n",
            "2337/2337 [==============================] - 292s 125ms/step - loss: 1.0010 - val_loss: 1.0469\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "The computer is a place to sit on the sun. The sun is a place to cook food.\n",
            "\n",
            "He put a baby in the fridge. A car is not a place to stay in a bathtub.\n",
            "\n",
            "The sea is a place to stay at the store. A computer is not a place to stay in a bath.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "The sun can make the course of it. The baseball water is not a person\n",
            "\n",
            "The car is the store to see a hunter. A bus is not a place to go to someone who don't have the conversation because it is in the barn to school. Ice causes the trees and sickness in the water.\n",
            "\n",
            "He put his bike and white her book. A children have no songs and therefore can not be cooked.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "he seen exercise to terrible the laptop. Food is instrumented with meters or things\n",
            "\n",
            "lions Read the North Pole First time to be in her ugly. hoses should be injured in a state\n",
            "\n",
            "Cooker is full two sen. people should eat tools\n",
            "\n",
            "Epoch 10/10\n",
            "2337/2337 [==============================] - 294s 126ms/step - loss: 0.9862 - val_loss: 1.0393\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "The baby can find a panda in the sky. A car is not a place to sleep in the sky.\n",
            "\n",
            "The sun rises from the sun. The sun is too hot to watch the sun\n",
            "\n",
            "The car is a place to stay in the sky. A bath is not a place to stay in a car.\n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "The tire of the book held a dog. People can't climb the car.\n",
            "\n",
            "A day can see the book with a bag. Cars do not have long teachers\n",
            "\n",
            "She drank a store. A car is not a single pillow.\n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Low-break is used to eat. Rharker's exercoids loves you know\n",
            "\n",
            "light a giraffe is too large to chat a bird\n",
            "\n",
            "The girl drove her snake. Pilots do not eat thrown.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH99Gz_ksULy",
        "colab_type": "text"
      },
      "source": [
        "### 6. Generate explanation why an input context makes sense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajkNxIlW47Cj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2aa89564-aa68-4080-cffb-b5d974ee1d04"
      },
      "source": [
        "# Initialise answers dictionary\n",
        "answers = dict()\n",
        "# For index and context in the test dataset\n",
        "for i, context in zip(train_data['id'][:100], train_data['FalseSent'][:100]):\n",
        "    # Cast the index as an integer\n",
        "    i = int(i)\n",
        "    # If the context does not end in a period add one\n",
        "    if context[-1] != '.':\n",
        "        context += '.'\n",
        "    # Occassionally the model does not return a prediction, this can be\n",
        "    # circumvented by continuing to make predictions until one is made\n",
        "    generated_texts = list()\n",
        "    while not generated_texts:\n",
        "        # Make prediction with the current context from dataset, the\n",
        "        # temperature represents the threshold set on the model on selecting\n",
        "        # sub-optimal predictions\n",
        "        generated_texts = textgen.generate(n=1, prefix=context,\n",
        "                                           temperature=0.7,\n",
        "                                           return_as_list=True,\n",
        "                                           max_gen_length=150)\n",
        "    # Extract the generated text by splitting on the period at the end of the\n",
        "    # input context\n",
        "    answer = generated_texts[0].split('.')[1]\n",
        "    # Add context and predicted text to answer dict\n",
        "    answers[str(i + 1)] = answer\n",
        "    # Output the first ten predictions\n",
        "    if i <= 10:\n",
        "        print('Context: {c} | Answer: {a}'.format(c=context, a=answer))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Context: He poured orange juice on his cereal. | Answer:  Boats are best programmed because they were not enough to be furniture\n",
            "Context: He drinks apple. | Answer:  Programming is not used for speaking for cancer\n",
            "Context: Jeff ran 100,000 miles today. | Answer:  walleting a clock is not a place for gold in a morning\n",
            "Context: I sting a mosquito. | Answer:  Leaves are not sold as the sun in a fridge care of air\n",
            "Context: A giraffe is a person. | Answer:  A cat gave his desk when they are dead\n",
            "Context: A normal closet is larger than a walk-in closet. | Answer:  A book has nothing to get the car only on modern the freezer\n",
            "Context: I like to ride my chocolate. | Answer:  Monkeys are not edible\n",
            "Context: A GIRL WON THE RACE WITH HORSE. | Answer:  The sea and used to chase it to make people use by seawater\n",
            "Context: he put elephant into the jug. | Answer:  Babies are a common thing\n",
            "Context: A dog plays volleyball. | Answer:  Smoothies do not enhance dead people\n",
            "Context: Eggs eat kis on Easter. | Answer:  Sports have best weeds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8pyGNQsskkp",
        "colab_type": "text"
      },
      "source": [
        "### 7. Evaluate accuracy of model using BLEU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh9Xx9xqC4pC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4df4f0d3-4745-45ed-b1ed-721c4bf6cd4c"
      },
      "source": [
        "def _get_ngrams(segment, max_order):\n",
        "    \"\"\"Extracts all n-grams upto a given maximum order from an input segment.\n",
        "    Args:\n",
        "        segment: text segment from which n-grams will be extracted.\n",
        "        max_order: maximum length in tokens of the n-grams returned by this\n",
        "        methods.\n",
        "    Returns:\n",
        "        The Counter containing all n-grams upto max_order in segment\n",
        "        with a count of how many times each n-gram occurred.\n",
        "    \"\"\"\n",
        "    ngram_counts = collections.Counter()\n",
        "    for order in range(1, max_order + 1):\n",
        "        for i in range(0, len(segment) - order + 1):\n",
        "            ngram = tuple(segment[i:i + order])\n",
        "            ngram_counts[ngram] += 1\n",
        "    return ngram_counts\n",
        "\n",
        "\n",
        "def _compute_bleu(reference_corpus, translation_corpus, max_order=4,\n",
        "                  smooth=False):\n",
        "    \"\"\"Computes BLEU score of translated segments against one or more references.\n",
        "    Args:\n",
        "        reference_corpus: list of lists of references for each translation. Each\n",
        "            reference should be tokenized into a list of tokens.\n",
        "        translation_corpus: list of translations to score. Each translation\n",
        "            should be tokenized into a list of tokens.\n",
        "        max_order: Maximum n-gram order to use when computing BLEU score.\n",
        "        smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
        "    Returns:\n",
        "        3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n",
        "            precisions and brevity penalty.\n",
        "    \"\"\"\n",
        "    matches_by_order = [0] * max_order\n",
        "    possible_matches_by_order = [0] * max_order\n",
        "    reference_length = 0\n",
        "    translation_length = 0\n",
        "    for (references, translation) in zip(reference_corpus, translation_corpus):\n",
        "        reference_length += min(len(r) for r in references)\n",
        "        translation_length += len(translation)\n",
        "\n",
        "        merged_ref_ngram_counts = collections.Counter()\n",
        "        for reference in references:\n",
        "            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n",
        "        translation_ngram_counts = _get_ngrams(translation, max_order)\n",
        "        overlap = translation_ngram_counts & merged_ref_ngram_counts\n",
        "        for ngram in overlap:\n",
        "            matches_by_order[len(ngram) - 1] += overlap[ngram]\n",
        "        for order in range(1, max_order + 1):\n",
        "            possible_matches = len(translation) - order + 1\n",
        "            if possible_matches > 0:\n",
        "                possible_matches_by_order[order - 1] += possible_matches\n",
        "\n",
        "    precisions = [0] * max_order\n",
        "    for i in range(0, max_order):\n",
        "        if smooth:\n",
        "            precisions[i] = ((matches_by_order[i] + 1.) /\n",
        "                             (possible_matches_by_order[i] + 1.))\n",
        "        else:\n",
        "            if possible_matches_by_order[i] > 0:\n",
        "                precisions[i] = (float(matches_by_order[i]) /\n",
        "                                 possible_matches_by_order[i])\n",
        "            else:\n",
        "                precisions[i] = 0.0\n",
        "\n",
        "    if min(precisions) > 0:\n",
        "        p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n",
        "        geo_mean = math.exp(p_log_sum)\n",
        "    else:\n",
        "        geo_mean = 0\n",
        "\n",
        "    ratio = float(translation_length) / reference_length\n",
        "\n",
        "    if ratio > 1.0:\n",
        "        bp = 1.\n",
        "    else:\n",
        "        bp = math.exp(1 - 1. / ratio)\n",
        "\n",
        "    bleu = geo_mean * bp\n",
        "\n",
        "    return (bleu, precisions, bp, ratio, translation_length, reference_length)\n",
        "\n",
        "\n",
        "def calculate_bleu(references: Dict[str, List[List[str]]],\n",
        "                   predictions: Dict[str, List[str]],\n",
        "                   max_order=4,\n",
        "                   smooth=False) -> float:\n",
        "    reference_corpus = []\n",
        "    prediction_corpus = []\n",
        "\n",
        "    for instance_id, reference_sents in references.items():\n",
        "        try:\n",
        "            prediction_sent = predictions[instance_id]\n",
        "        except KeyError:\n",
        "            logging.error(\"Missing prediction for instance '%s'.\", instance_id)\n",
        "            sys.exit(EXIT_STATUS_PREDICTION_MISSING)\n",
        "\n",
        "        del predictions[instance_id]\n",
        "\n",
        "        prediction_corpus.append(prediction_sent)\n",
        "        reference_corpus.append(reference_sents)\n",
        "\n",
        "    if len(predictions) > 0:\n",
        "        logging.error(\"Found %d extra predictions, for example: %s\",\n",
        "                      len(predictions),\n",
        "                      \", \".join(list(predictions.keys())[:3]))\n",
        "        sys.exit(EXIT_STATUS_PREDICTIONS_EXTRA)\n",
        "\n",
        "    score = _compute_bleu(reference_corpus, prediction_corpus,\n",
        "                          max_order=max_order, smooth=smooth)[0]\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "def read_references(filename: str) -> List[List[List[str]]]:\n",
        "    references = {}\n",
        "    with open(filename, \"rt\", encoding=\"UTF-8\", errors=\"replace\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        try:\n",
        "            count = 0\n",
        "            for row in reader:\n",
        "                if count < 100:\n",
        "                    try:\n",
        "                        instance_id = row[0]\n",
        "                        references_raw1 = row[1]\n",
        "                        references_raw2 = row[2]\n",
        "                        references_raw3 = row[3]\n",
        "                    except IndexError as e:\n",
        "                        logging.error(\n",
        "                            \"Error reading value from CSV file %s on line %d: %s\",\n",
        "                            filename, reader.line_num, e)\n",
        "                        sys.exit(EXIT_STATUS_ANSWERS_MALFORMED)\n",
        "\n",
        "                    if instance_id in references:\n",
        "                        logging.error(\"Key %s repeated in file %s on line %d\",\n",
        "                                      instance_id, filename, reader.line_num)\n",
        "                        sys.exit(EXIT_STATUS_ANSWERS_MALFORMED)\n",
        "\n",
        "                    if instance_id == \"\":\n",
        "                        logging.error(\n",
        "                            \"Key is empty in file %s on line %d\", filename,\n",
        "                            reader.line_num)\n",
        "                        sys.exit(EXIT_STATUS_ANSWERS_MALFORMED)\n",
        "\n",
        "                    tokens = []\n",
        "                    for ref in [references_raw1, references_raw2,\n",
        "                                references_raw3]:\n",
        "                        if ref:\n",
        "                            tokens.append(ref.split())\n",
        "\n",
        "                    if len(tokens) == 0:\n",
        "                        logging.error(\n",
        "                            \"No reference sentence in file %s on line %d\",\n",
        "                            filename, reader.line_num)\n",
        "                        sys.exit(EXIT_STATUS_ANSWERS_MALFORMED)\n",
        "\n",
        "                    references[instance_id] = tokens\n",
        "                count += 1\n",
        "\n",
        "        except csv.Error as e:\n",
        "            logging.error('file %s, line %d: %s', filename, reader.line_num, e)\n",
        "            sys.exit(EXIT_STATUS_ANSWERS_MALFORMED)\n",
        "\n",
        "    return references\n",
        "\n",
        "\n",
        "references = read_references(test_answers_path)\n",
        "bleu = calculate_bleu(references, answers,\n",
        "                      max_order=4, smooth=True)\n",
        "\n",
        "print(f'BLEU score: {bleu * 100:.4f}.')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU score: 0.0712.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7iTDh0Gsyir",
        "colab_type": "text"
      },
      "source": [
        "### 8. Results: Task 3 - Explanation (Generation)\n",
        "\n",
        "The results from task 3 were very disappointing, the BLEU accuracy score was less than 0.1 and the generated sentences in every instance were non-sensical and only on rare occasions have any connection to the input context.\n",
        "\n",
        "Example:\n",
        "\n",
        "`Context: A dog plays volleyball. | Answer:  Smoothies do not enhance dead people`\n",
        "\n",
        "The poor performance of the character level RNN is likely due to the lack of training on a suitable corpus. Whilst other NLU/NLP models such as BERT or GPT2 have been trained on extensive corpora, the character level RNN implemented in task-3 was trained on a subsection of the website Reddit. It is my opinion that to best approach this task the model would need to be trained on a body of text such as Wikipedia, or a collection of encyclopaedias, so it can start to understand the connections between words and learn deeper contexts.\n",
        "\n",
        "Additional training and fine-tuning were carried out on the char-RNN to improve its accuracy, but after 10 epochs it was still not possible to generate any reasonable answers. It is worth noting that an attempt was made to extend the learning on the training data by increasing the number of epochs, but this resulted in a GPU restriction placed on my Colab account due to unfair usage of GPU resources which has still not been lifted at the time of writing.  It is assumed that more training epochs will results in better text generation, but this remains untested with the current dataset.\n",
        "\n",
        "BERT was not chosen for this task because of its unsuitability for next word/character tasks due to its masked bidirectional nature. There is a possibility for further research into masking entire sentences for BERT inputs and predicting words/sentences in that manner, but it is outside of the scope of this assignment and the time frame available for such a body of work.\n",
        "\n"
      ]
    }
  ]
}