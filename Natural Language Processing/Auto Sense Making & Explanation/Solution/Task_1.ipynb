{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Task-1.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "pfTmo74gV_73",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# -----------------------------------------------------\n",
    "# Natural Language Processing\n",
    "# Assignment 2 - Automatic Sense Making and Explanation\n",
    "# Task 1 - Validation\n",
    "# Michael McAleer R00143621\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Note: This has been run and tested on Python 3.6 with TensorFlow 1.15.0"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NT3BtCWUmO_3",
    "colab_type": "text"
   },
   "source": [
    "### 1. Install package dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rKCbhcFLlOZQ",
    "colab_type": "code",
    "outputId": "aacfce63-d7fd-4dc3-ac05-227332f803cd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    }
   },
   "source": [
    "!pip install bert-tensorflow"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collecting bert-tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
      "\r\u001b[K     |████▉                           | 10kB 25.2MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
      "Installing collected packages: bert-tensorflow\n",
      "Successfully installed bert-tensorflow-1.0.1\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S90iyjQpmSLo",
    "colab_type": "text"
   },
   "source": [
    "### 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-Qwm5YJelZVY",
    "colab_type": "code",
    "outputId": "b7e37b5e-8d35-47e1-fcfe-9f53ed562e77",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    }
   },
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow.python.keras as keras\n",
    "\n",
    "from bert.tokenization import FullTokenizer\n",
    "from tqdm import tqdm_notebook"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rg0mrcydmW3f",
    "colab_type": "text"
   },
   "source": [
    "### 3. Set paths, download dependencies, set constants"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wlzhNW0Hlbo-",
    "colab_type": "code",
    "outputId": "3278af1d-763c-4eb3-fb44-7047da5526c6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    }
   },
   "source": [
    "# Set root path dependent on system\n",
    "# If System is Windows, set ROOT_DIR as current working directory\n",
    "if os.name == 'nt':\n",
    "    ROOT_DIR = os.getcwd()\n",
    "# Else running on CoLab, set ROOT_DIR to match environment path\n",
    "else:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')\n",
    "    ROOT_DIR = '/content/drive/My Drive/Colab Notebooks/'\n",
    "\n",
    "# Paths to data and model output dir\n",
    "DATA_DIR = '{root}/data'.format(root=ROOT_DIR)\n",
    "TRAIN_DIR = '{data}/train'.format(data=DATA_DIR)\n",
    "TEST_DIR = '{data}/test'.format(data=DATA_DIR)\n",
    "\n",
    "# Params for bert model and tokenisation\n",
    "BERT_PATH = 'https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1'\n",
    "MAX_SEQ_LENGTH = 256\n",
    "\n",
    "# Initialize TensorFlow session\n",
    "session = tf.Session()"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DS_LUbZAmYmc",
    "colab_type": "text"
   },
   "source": [
    "### 4. Import training and test data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gASxtZGFlfpf",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Set training data path\n",
    "training_data_path = '{td}/subtaskA_data_all.csv'.format(td=TRAIN_DIR)\n",
    "training_answers_path = '{td}/subtaskA_answers_all.csv'.format(td=TRAIN_DIR)\n",
    "# Load training data\n",
    "train_data = pd.read_csv(training_data_path, index_col='id')\n",
    "train_answers = pd.read_csv(training_answers_path,\n",
    "                            names=['id', 'most_unlikely'], index_col='id')\n",
    "\n",
    "# Set test data path\n",
    "test_data_path = '{td}/taskA_trial_data.csv'.format(td=TEST_DIR)\n",
    "test_answers_path = '{td}/taskA_trial_answer.csv'.format(td=TEST_DIR)\n",
    "# Load test data\n",
    "test_data = pd.read_csv(test_data_path, index_col='id')\n",
    "test_answers = pd.read_csv(training_answers_path,\n",
    "                           names=['id', 'most_unlikely'], index_col='id')\n",
    "\n",
    "# Add labels to the train and test data frames\n",
    "train_data['most_unlikely'] = train_answers['most_unlikely']\n",
    "test_data['most_unlikely'] = test_answers['most_unlikely']"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vd2lynRzma2-",
    "colab_type": "text"
   },
   "source": [
    "### 5. Clean data before bert pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9cgnhEo7ljRd",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def clean_raw_data(data):\n",
    "    \"\"\"Given raw input data, normalise to lower case and remove all\n",
    "    punctuation from the string.\n",
    "\n",
    "    :param data: raw data -- pandas dataframe\n",
    "    :return: normalised data -- pandas dataframe\n",
    "    \"\"\"\n",
    "    # For each data frame column holding a sentence...\n",
    "    for col in ['sent0', 'sent1']:\n",
    "        # Normalise case...\n",
    "        data[col] = data[col].apply(\n",
    "            lambda x: ' '.join(w.lower() for w in x.split()))\n",
    "        # Remove any punctuation or symbols...\n",
    "        data[col] = data[col].str.replace(r'[^\\w\\s]', '')\n",
    "    return data\n",
    "\n",
    "\n",
    "# Clean both the training and test data - the model used is uncased so it is\n",
    "# necessary to have our input data also uncased\n",
    "train_data = clean_raw_data(train_data)\n",
    "test_data = clean_raw_data(test_data)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rjrfTsXmcnt",
    "colab_type": "text"
   },
   "source": [
    "### 6. Split training data into training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c1GhZOsHlk07",
    "colab_type": "code",
    "outputId": "00825531-a0d1-4fb9-a81d-34ede4a9f33b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    }
   },
   "source": [
    "# Split -- 95%:5% \n",
    "train_size = train_data.shape[0]\n",
    "split = int((train_size / 100) * 95)\n",
    "train_split = train_data[:split]\n",
    "val_split = train_data[split:]\n",
    "\n",
    "# Output details of split process and new training dataset size\n",
    "print('Initial training data size: {size}'.format(size=train_size))\n",
    "print('Training Data Size: {size}'.format(size=train_split.shape[0]))\n",
    "print('Validation Data: {size}'.format(size=val_split.shape[0]))"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Initial training data size: 10000\n",
      "Training Data Size: 9500\n",
      "Validation Data: 500\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKRnBfWFme7N",
    "colab_type": "text"
   },
   "source": [
    "### 7. Prepare data for conversion for Bert use"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1BBqSj16lmcf",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def transform_data_for_bert_use(data):\n",
    "    \"\"\"Take normalised input data frame, reduce the sequence length to half\n",
    "    of the maxiumum sequence length -2 to allow for two large sentences and\n",
    "    the required tags for bert to be added (three in total).\n",
    "\n",
    "    :param data: normalised data -- pandas dataframe\n",
    "    :return: sentence0, sentence1, labels -- np.array, np.array, list\n",
    "    \"\"\"\n",
    "    def _inner_prep(data_column):\n",
    "        # Convert the column to a list\n",
    "        d_s = data[data_column].to_list()\n",
    "        # Trim each of the column values to (max seq / 2) - 2) if they exceed\n",
    "        # that size\n",
    "        d_s = [' '.join(\n",
    "            t.split()[0:int((MAX_SEQ_LENGTH / 2) - 2)]) for t in d_s]\n",
    "        # Convert the list to a numpy array and convert to 2D\n",
    "        d_s = np.array(d_s, dtype=object)[:, np.newaxis]\n",
    "        return d_s\n",
    "\n",
    "    return (_inner_prep('sent0'), _inner_prep('sent1'),\n",
    "            data['most_unlikely'].tolist())\n",
    "\n",
    "\n",
    "# Transform the training, validation and test data so it ready to become an\n",
    "# input example and later converted to Bert feature\n",
    "train_s0, train_s1, train_labels = transform_data_for_bert_use(train_split)\n",
    "val_s0, val_s1, val_labels = transform_data_for_bert_use(val_split)\n",
    "test_s0, test_s1, test_labels = transform_data_for_bert_use(test_data)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1LhY576mgxM",
    "colab_type": "text"
   },
   "source": [
    "### 8. Get the Bert tokenizer from TensorFlow Hub"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0uElfUO9loNo",
    "colab_type": "code",
    "outputId": "68b17465-e23a-44c4-a769-9bd929aaa4ec",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    }
   },
   "source": [
    "def create_bert_tokeniser():\n",
    "    \"\"\"Get the Bert tokeniser for the Bert model in use in this task,\n",
    "    specifically the lower case vocabulary set.\n",
    "\n",
    "    :return: Bert Tokeniser\n",
    "    \"\"\"\n",
    "    # Define the Bert module in use\n",
    "    bert_module = hub.Module(BERT_PATH)\n",
    "    # Get the Bert tokeniser info\n",
    "    tokeniser_info = bert_module(signature='tokenization_info',\n",
    "                                 as_dict=True)\n",
    "    # Get the lower case Bert tokeniser vocabulary\n",
    "    vocab_file, do_lower_case = session.run([\n",
    "        tokeniser_info['vocab_file'], tokeniser_info['do_lower_case']])\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "\n",
    "# Instantiate tokenizer\n",
    "bert_tokeniser = create_bert_tokeniser()"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTZFA2TNmiyd",
    "colab_type": "text"
   },
   "source": [
    "### 9. Create Bert Input 'Examples'"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ADkvzg4SlqUQ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single Bert input 'example'.\"\"\"\n",
    "\n",
    "    def __init__(self, text_a, text_b, label=None):\n",
    "        \"\"\"Construct an input example.\n",
    "\n",
    "        :param text_a: The raw text of the first sentence -- str\n",
    "        :param text_b: The raw text of the second sentence -- str\n",
    "        :param label: The label of the example -- int [0, 1]\n",
    "        \"\"\"\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "def convert_data_to_examples(sent0, sent1, labels):\n",
    "    \"\"\"Create input examples from entire input dataset.\n",
    "\n",
    "    :param sent0: sentence 0 values -- list\n",
    "    :param sent1: sentence 1 values -- list\n",
    "    :param labels: sentence label -- list\n",
    "    :return: InputExamples -- list\n",
    "    \"\"\"\n",
    "    input_examples = list()\n",
    "    # For each of the rows in the data\n",
    "    for s0, s1, label in zip(sent0, sent1, labels):\n",
    "        # Convert to InputExample object and add to list of examples\n",
    "        input_examples.append(\n",
    "            InputExample(text_a=' '.join(s0),\n",
    "                         text_b=' '.join(s1),\n",
    "                         label=label))\n",
    "\n",
    "    return input_examples\n",
    "\n",
    "\n",
    "# Convert data to InputExample format\n",
    "train_examples = convert_data_to_examples(train_s0, train_s1, train_labels)\n",
    "val_examples = convert_data_to_examples(val_s0, val_s1, val_labels)\n",
    "test_examples = convert_data_to_examples(test_s0, test_s1, test_labels)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PejbD1ytmku2",
    "colab_type": "text"
   },
   "source": [
    "### 10. Convert input examples to Bert features"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dbC2dOjSluC3",
    "colab_type": "code",
    "outputId": "78fa04c2-c92a-4979-cb0d-1430e8178c84",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    }
   },
   "source": [
    "def convert_examples_to_features(tokeniser, examples, dataset):\n",
    "    \"\"\"Convert input examples into features ready for input into Bert using the\n",
    "    loaded tokeniser.\n",
    "\n",
    "    :param tokeniser: Bert tokeniser -- obj\n",
    "    :param examples: input examples -- list\n",
    "    :param dataset: dataset being converted -- str\n",
    "    :return: Bert features (input IDs, input masks, segment IDs, labels)\n",
    "             -- (np.array, np.array, np.array, np.array)\n",
    "    \"\"\"\n",
    "    # Initialise lists to hold input ids, masks, segments, and labels\n",
    "    i_ids, i_masks, s_ids, lbls = list(), list(), list(), list()\n",
    "    # Message to return from progress bar\n",
    "    msg = 'Feature conversion of {d} dataset in progress...'.format(d=dataset)\n",
    "    # For each of the examples in the dataset (track progress with tqdm lib)\n",
    "    for example in tqdm_notebook(examples, desc=msg):\n",
    "        # Convert the example to a Bert feature\n",
    "        i_id, i_mask, s_id, lbl = convert_single_example(tokeniser, example)\n",
    "        # Append the feature values to the respective lists\n",
    "        i_ids.append(i_id)\n",
    "        i_masks.append(i_mask)\n",
    "        s_ids.append(s_id)\n",
    "        lbls.append(lbl)\n",
    "\n",
    "    return (np.array(i_ids), np.array(i_masks),\n",
    "            np.array(s_ids), np.array(lbls).reshape(-1, 1))\n",
    "\n",
    "\n",
    "def convert_single_example(tokeniser, example):\n",
    "    \"\"\"Convert an input example into a Bert feature.\n",
    "\n",
    "    :param tokeniser: Bert tokeniser -- obj\n",
    "    :param example: input example -- obj\n",
    "    :return: input ids, input mask, segment IDs, label -- list, list, list, int\n",
    "    \"\"\"\n",
    "    # Tokenise sentence 0 with Bert tokeniser\n",
    "    tokens_a = tokeniser.tokenize(example.text_a)\n",
    "    # Tokenise sentence 1 with Bert tokeniser\n",
    "    tokens_b = tokeniser.tokenize(example.text_b)\n",
    "    # Initialise list to hold tokens and tags, and another for the segment IDs\n",
    "    tokens, segment_ids = list(), list()\n",
    "    # Start the tokens with the Bert sentence start tag [CLS]\n",
    "    tokens.append('[CLS]')\n",
    "    # Append 0 to segment IDs to indicate start of sentence 0\n",
    "    segment_ids.append(0)\n",
    "    # For each of the tokens in sentence 0\n",
    "    for token in tokens_a:\n",
    "        # Append the word token to the list of tokens\n",
    "        tokens.append(token)\n",
    "        # Append 0 to the list of segment IDs\n",
    "        segment_ids.append(0)\n",
    "    # After all sentence 0 tokens are processed add the sentence seperator\n",
    "    # tag [SEP]\n",
    "    tokens.append('[SEP]')\n",
    "    # Append 0 to the list of segment IDs for the seperator tag, this is a Bert\n",
    "    # model requirement\n",
    "    segment_ids.append(0)\n",
    "    # For each of the word tokens in sentence 1\n",
    "    for token in tokens_b:\n",
    "        # Append the word token to the list of tokens\n",
    "        tokens.append(token)\n",
    "        # Append 1 to the list of segment IDs to indicate second sentence\n",
    "        segment_ids.append(1)\n",
    "    # After all sentence 1 tokens are processed add the [CLS] tag to indicate\n",
    "    # the end of the sentence\n",
    "    tokens.append('[CLS]')\n",
    "    # Append 1 to the segment IDs for the last token in the sequence\n",
    "    segment_ids.append(1)\n",
    "    # Convert the list of tokens to a list of integers indicating their index\n",
    "    # in the Bert model vocabulary\n",
    "    input_ids = tokeniser.convert_tokens_to_ids(tokens)\n",
    "    # Create a mask equal to the length of the tokens with value 1 in each\n",
    "    # position to tell Bert what tokens to pay attention to, 0 will be ignored\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    # Pad each of the sequences with 0 values so they are equal in length to\n",
    "    # the maximum sequence length of 256\n",
    "    while len(input_ids) < MAX_SEQ_LENGTH:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "    # Assert each of the sequences are equal to the maximum sequence length\n",
    "    assert len(input_ids) == MAX_SEQ_LENGTH\n",
    "    assert len(input_mask) == MAX_SEQ_LENGTH\n",
    "    assert len(segment_ids) == MAX_SEQ_LENGTH\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n",
    "\n",
    "# Convert the training input examples into Bert input features\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels) = (\n",
    "    convert_examples_to_features(\n",
    "        bert_tokeniser, train_examples, dataset='Training'))\n",
    "# Convert the valdation input examples into Bert input features\n",
    "(val_input_ids, val_input_masks, val_segment_ids, val_labels) = (\n",
    "    convert_examples_to_features(\n",
    "        bert_tokeniser, val_examples, dataset='Validation'))\n",
    "# Convert the test input examples into Bert input features\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels) = (\n",
    "    convert_examples_to_features(\n",
    "        bert_tokeniser, test_examples, dataset='Test'))"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513a98e6c057438e8edb802521a6d45a",
       "version_minor": 0,
       "version_major": 2
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Feature conversion of Training dataset in progress...', max=9…"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421f04ec5a1c4bcab034610dd47715a2",
       "version_minor": 0,
       "version_major": 2
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Feature conversion of Validation dataset in progress...', max…"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eed1f9452044c7e908d5423e496b8ed",
       "version_minor": 0,
       "version_major": 2
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Feature conversion of Test dataset in progress...', max=2021,…"
      ]
     },
     "metadata": {
      "tags": []
     }
    },
    {
     "output_type": "stream",
     "text": [
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEVMDCbgmneg",
    "colab_type": "text"
   },
   "source": [
    "### 11. Transfer the Bert model from TensorFlow Hub and fine-tune for task"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LHL2xQSzlwsl",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"The transferred Bert Model (uncased/large - L-12_H-768_A-12).\"\"\"\n",
    "\n",
    "    def __init__(self, n_fine_tune_layers=3, **kwargs):\n",
    "        \"\"\"Initialise the Bert layer parameters.\n",
    "\n",
    "        :param n_fine_tune_layers: How many layers of Bert model to\n",
    "                                   fine-tune -- int\n",
    "        :param kwargs: additional key-word arguements\n",
    "        \"\"\"\n",
    "        # Amount of layers to fine tune\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        # Is model trainable\n",
    "        self.trainable = True\n",
    "        # Model output size\n",
    "        self.output_size = 768\n",
    "        # Path to Bert model in TF Hub\n",
    "        self.bert_path = BERT_PATH\n",
    "        # Super() the BertLayer\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Build the Bert model.\n",
    "        \n",
    "        :param input_shape: input feature shape -- int\n",
    "        \"\"\"\n",
    "        # Load trainable Bert model\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable,\n",
    "            name='bert_projectA_module')\n",
    "        # Set model trainable layers\n",
    "        trainable_layers = ['pooler/dense']\n",
    "        # Remove unused layers\n",
    "        trainable_vars = [\n",
    "            var for var in self.bert.variables if '/cls/' not in var.name]\n",
    "        # Select all the layers for fine-tuning\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append('encoder/layer_{x}'.format(x=str(11 - i)))\n",
    "        # Update trainable variables to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var for var in trainable_vars if (\n",
    "                any([l in var.name for l in trainable_layers]))]\n",
    "        # Add trainable variables to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "        # Add non-trainable variables to non-trainable weights\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "        # Super() the Bert Layer to build with the input shape\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"Called after build, apply layers to input tensors.\n",
    "\n",
    "        :param inputs: Bert model feature inputs -- tuple\n",
    "        :return: Bert model -- TensorFlow model\n",
    "        \"\"\"\n",
    "        # Cast all the inputs as integers\n",
    "        inputs = [keras.backend.cast(x, dtype='int32') for x in inputs]\n",
    "        # Extract layer inputs\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        # Create input feature dict\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask,\n",
    "            segment_ids=segment_ids)\n",
    "        # Return Bert model\n",
    "        return self.bert(inputs=bert_inputs, signature='tokens',\n",
    "                         as_dict=True)['pooled_output']\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"Shape transformation logic.\n",
    "\n",
    "        :param input_shape: input shape -- int\n",
    "        :return: input shape, output size -- int, int\n",
    "        \"\"\"\n",
    "        return input_shape[0], self.output_size"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhhITG9emp-W",
    "colab_type": "text"
   },
   "source": [
    "### 12. Build & Train Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QR-LqegwlzNv",
    "colab_type": "code",
    "outputId": "eda973a9-fecb-4eb3-f309-c810012e8b93",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 819
    }
   },
   "source": [
    "def build_model():\n",
    "    \"\"\"Build the complete Bert transfer model with fine-tuning.\n",
    "\n",
    "    :return: Bert model -- Keras Model\n",
    "    \"\"\"\n",
    "    # Define input layer to hold feature input IDs\n",
    "    in_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,),\n",
    "                                  name='input_ids')\n",
    "    # Define input layer to hold feature input masks\n",
    "    in_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,),\n",
    "                                    name='input_masks')\n",
    "    # Define input layer to hold feature input segment IDs\n",
    "    in_segment = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,),\n",
    "                                       name='segment_ids')\n",
    "    # Colate all input layers\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "    # A number of attempts made to further fine-tune and reduce over fitting\n",
    "    # on training data, ultimately this was abandoned due to the impact on\n",
    "    # accuracy whilst not having the same noticeable impact on loss reduction\n",
    "\n",
    "    # Add the Bert model layer and the amount of layers to fine-tune, the\n",
    "    # bert inputs will serve as the inputs into the Bert Layer\n",
    "    bert_output = BertLayer(n_fine_tune_layers=3)(bert_inputs)\n",
    "    # Add a dropout layer between the Bert model and the first fully connected\n",
    "    # dense layer\n",
    "    drop = tf.keras.layers.Dropout(0.1)(bert_output)\n",
    "    # Add a fully connected dense layer with 256 neurons and relu activation\n",
    "    # function\n",
    "    dense = tf.keras.layers.Dense(256, activation='relu')(drop)\n",
    "\n",
    "    # drop1 = tf.keras.layers.Dropout(0.2)(bert_output)\n",
    "    # dense1 = tf.keras.layers.Dense(256, activation='relu')(drop1)\n",
    "    # drop2 = tf.keras.layers.Dropout(0.1)(dense1)\n",
    "    # dense2 = tf.keras.layers.Dense(128, activation='relu')(drop2)\n",
    "\n",
    "    # Add a softmax classifier with 2 neurons to represent [0, 1] labels\n",
    "    pred = tf.keras.layers.Dense(2, activation='softmax')(dense)\n",
    "    # Initialise the Keras model, define the input layer and output layer, the\n",
    "    # defined layers above handle the inter-connects\n",
    "    bert_model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
    "    bert_adam = keras.optimizers.Adam(lr=2e-5, decay=0.001)\n",
    "\n",
    "    # bert_model.compile(loss='binary_crossentropy', optimizer=bert_adam,\n",
    "    #                    metrics=['accuracy'])\n",
    "\n",
    "    # Compile the model with loss function, optimiser and return accuracy\n",
    "    bert_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                       optimizer=bert_adam, metrics=['accuracy'])\n",
    "    # Output the model summary\n",
    "    bert_model.summary()\n",
    "\n",
    "    return bert_model\n",
    "\n",
    "\n",
    "def initialise_session(s):\n",
    "    \"\"\"Iniitialise all the TensorFlow session variables.\n",
    "\n",
    "    :param s: TensorFlow session -- obj\n",
    "    \"\"\"\n",
    "    # Initialise local variables\n",
    "    s.run(tf.local_variables_initializer())\n",
    "    # Initialise global variables\n",
    "    s.run(tf.global_variables_initializer())\n",
    "    # Initialise TensorFlow tables\n",
    "    s.run(tf.tables_initializer())\n",
    "    # Set session as Keras backend session\n",
    "    keras.backend.set_session(s)\n",
    "\n",
    "\n",
    "# Initialise the Bert model\n",
    "model = build_model()\n",
    "# Instantiate variables\n",
    "initialise_session(session)\n",
    "# Fit the model on the training data and validate on the validation data\n",
    "model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids], train_labels,\n",
    "    validation_data=(\n",
    "        [val_input_ids, val_input_masks, val_segment_ids], val_labels),\n",
    "    epochs=3, batch_size=32)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer (BertLayer)          (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 768)          0           bert_layer[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            514         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 110,302,268\n",
      "Trainable params: 22,051,586\n",
      "Non-trainable params: 88,250,682\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Train on 9500 samples, validate on 500 samples\n",
      "Epoch 1/3\n",
      "9500/9500 [==============================] - 474s 50ms/sample - loss: 0.6291 - acc: 0.6211 - val_loss: 0.6657 - val_acc: 0.6380\n",
      "Epoch 2/3\n",
      "9500/9500 [==============================] - 471s 50ms/sample - loss: 0.3833 - acc: 0.8222 - val_loss: 0.6275 - val_acc: 0.6840\n",
      "Epoch 3/3\n",
      "9500/9500 [==============================] - 470s 49ms/sample - loss: 0.2623 - acc: 0.8896 - val_loss: 0.7023 - val_acc: 0.6920\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1b4b707048>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 12
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJXrEWammzk_",
    "colab_type": "text"
   },
   "source": [
    "### 13. Make predictions on the most unlikely sentence in the test data set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8awaMDukl01U",
    "colab_type": "code",
    "outputId": "4187b6f6-49b1-4c23-a92f-9bf5513adb58",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "# Make predictions on the test dataset values\n",
    "predictions = model.predict([test_input_ids,\n",
    "                             test_input_masks,\n",
    "                             test_segment_ids])\n",
    "\n",
    "# Get the predicted label for each test dataset input\n",
    "predictions_max = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Create answer dictionary for use by evaluation tool\n",
    "answers = dict()\n",
    "for i, pred in enumerate(predictions_max):\n",
    "    answers[str(i + 1)] = str(pred)\n",
    "\n",
    "# Output total predicted answer count\n",
    "print(len(answers.keys()))"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xC-UVAVm3xX",
    "colab_type": "text"
   },
   "source": [
    "### 14. Evaluate Results against gold labels"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GcKm9Zgxl25S",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Evaluation tools taken from competition website\n",
    "# There is no need to output the predictions to CSV to load them again, we can\n",
    "# pass in the expected prediction dictionary directly\n",
    "\n",
    "import csv\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def read_gold(filename):\n",
    "    answers = {}\n",
    "\n",
    "    with open(filename, \"rt\", encoding=\"UTF-8\", errors=\"replace\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)\n",
    "        try:\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    instance_id = row[0]\n",
    "                    answer = row[1]\n",
    "                except IndexError as e:\n",
    "                    logging.error(\n",
    "                        \"Error reading value from CSV file %s on line %d: %s\",\n",
    "                        filename, reader.line_num, e)\n",
    "                    sys.exit(EXIT_STATUS_ANSWERS_MALFORMED)\n",
    "\n",
    "                if instance_id in answers:\n",
    "                    logging.error(\"Key %s repeated in %s\",\n",
    "                                  instance_id, filename)\n",
    "                    sys.exit(EXIT_STATUS_ANSWERS_MALFORMED)\n",
    "\n",
    "                answers[instance_id] = answer\n",
    "\n",
    "        except csv.Error as e:\n",
    "            logging.error('file %s, line %d: %s', filename, reader.line_num, e)\n",
    "            sys.exit(EXIT_STATUS_ANSWERS_MALFORMED)\n",
    "\n",
    "    if len(answers) == 0:\n",
    "        logging.error(\"No answers found in file %s\", filename)\n",
    "        sys.exit(EXIT_STATUS_ANSWERS_MALFORMED)\n",
    "\n",
    "    return answers\n",
    "\n",
    "\n",
    "def calculate_accuracy(gold_labels, predictions):\n",
    "    score = 0.0\n",
    "\n",
    "    for instance_id, answer in gold_labels.items():\n",
    "        try:\n",
    "            predictions_for_current = predictions[instance_id]\n",
    "        except KeyError:\n",
    "            logging.error(\"Missing prediction for question '%s'.\", instance_id)\n",
    "            sys.exit(EXIT_STATUS_PREDICTION_MISSING)\n",
    "\n",
    "        if answer == predictions_for_current:\n",
    "            score += 1.0 / len(predictions_for_current)\n",
    "\n",
    "        del predictions[instance_id]\n",
    "\n",
    "    if len(predictions) > 0:\n",
    "        logging.error(\"Found %d extra predictions, for example: %s\", len(\n",
    "            predictions), \", \".join(list(predictions.keys())[:3]))\n",
    "        sys.exit(EXIT_STATUS_PREDICTIONS_EXTRA)\n",
    "\n",
    "    return score / len(gold_labels)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ufXuezUvl4BO",
    "colab_type": "code",
    "outputId": "b4d86139-80a4-45c2-9325-76db0f149115",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "gold_labels = read_gold(test_answers_path)\n",
    "accuracy = calculate_accuracy(gold_labels, answers)\n",
    "print(f'Accuracy: {accuracy * 100:.4f}%')"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Accuracy: 73.4785%\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtZuikXcTqMC",
    "colab_type": "text"
   },
   "source": [
    "### 15. Task-1 Results\n",
    "\n",
    "The validation task of sense making was undertaken using Google’s BERT model, specifically the uncased large model with 12 layers due to hardware constraints posed by the Colab platform.\n",
    "\n",
    "Sentence pairs with an associated label indicating the most unlikely sentence in the pair were input into the BERT model after they had been transformed and embedded into the correct format required for BERT.\n",
    "\n",
    "Example:\n",
    "\n",
    "[CLS]He put a turkey into the fridge.[SEP]He put an elephant into the fridge.[CLS] [1]\n",
    "\n",
    "During the training process a variation of model architectures were implemented to gauge the impact on accuracy and loss. These variations \n",
    "included\n",
    "\n",
    "•\tAdding fully connected dense layers \n",
    "\n",
    "•\tAdding dropout between the layers\n",
    "\n",
    "•\tEnabling layers in the base BERT model to become trainable\n",
    "\n",
    "•\tVarying the number of epochs that the BERT model is fine-tuned for\n",
    "\n",
    "Training BERT on the training data presented a model which was prone to overfitting extremely quickly, after just three epochs the validation loss started to increase at a very fast rate with no improvement seen in accuracy.\n",
    "To attempt to alleviate the impact of overfitting on the training data additional dense layers with varying rates of dropout were added both before and after the BERT layers but ultimately just a single dropout layer with a rate of 0.1 and a single fully connected layer after the BERT layers provided the best results. It was not possible to reduce overfitting by other traditional methods such as capacity reduction in the hidden layers as we do not want to negatively impact the quality of BERT. \n",
    "\n",
    "On the validation data the model obtained a loss of 0.7023 and accuracy of 0.6920 (this was seen to go as high as 0.71 in previous runs), on the test dataset an accuracy of 0.7347 was achieved. Whilst not particularly high accuracy values, they do match or exceed the results achieved by the competition organisers [1]. \n",
    "\n",
    "It is my opinion that with further time spent fine-tuning the model and a change in the input format of the sentences this number could be further increased. Implementing regularization on the weights may also have a positive impact on the rate of overfitting. After achieving good results in task-2 by creating more training samples by having an input for every answer in the dataset, a similar approach could be tried here where one sentence at a time input into BERT with a classification label for that one input. This format would double the size of the dataset and potentially improve overall accuracy and loss of the model after training. However, it is also taken into consideration that BERT was built to ‘understand’ sentence pairs so this assumption on inputting single tokens may not be valid.\n",
    "\n",
    "[1] Cunxiang Wang, Shuailong Liang , Yue Zhang , Xiaonan Li and Tian Gao. Does It Make Sense? And Why? A Pilot Study for Sense Making and Explanation. https://arxiv.org/pdf/1906.00363.pdf\n"
   ]
  }
 ]
}